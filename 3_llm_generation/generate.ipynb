{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you indend to run this on a server as pythong script you can run `python generate_server.py` instead.\n",
    "### However please edit the relevant values in ### SELECT ###, e.g. chose model index, org_folder index and stats index.\n",
    "## Import required modules\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from api_handler import ollama_req, openai_req, anthropic_req\n",
    "\n",
    "# Every prompt method is defined in this json dictonary according to the prompt methods design and can be found in ./2_prompt_engineering folder\n",
    "PROMPT_TEMPLATE = '1prompt_templates_system_stats.json'\n",
    "\n",
    "# Shorthand dict for prompt methods - Additional ones can be added but this should be not be touched in general and is automatically iterated over. Each of these prompt methods must be found and specified in the \n",
    "prompt_folders = {\n",
    "    'chain_of_thought':'cot',                       #0\n",
    "    'skeleton_of_thought':'sot',                    #1\n",
    "    'self_consistency':'sc',                        #2\n",
    "    'generated_knowledge':'gk',                     #3\n",
    "    'least_to_most':'ltm',                          #4\n",
    "    'chain_of_verification':'cov',                  #5\n",
    "    'step_back_prompting':'sbp',                    #6\n",
    "    'rephrase_and_respond':'rar',                   #7\n",
    "    'emotion_prompt':'em',                          #8\n",
    "    'directional_stimuli':'ds',                     #9\n",
    "    'recursive_criticism_and_improvement':'rcai',   #10\n",
    "    'reverse_prompting':'rp'                        #11\n",
    "}\n",
    "\n",
    "#################################################################################################\n",
    "##############################            SELECT          #######################################\n",
    "#################################################################################################\n",
    "# Run generation of synthetic data prompts N times, default is 15 for research. In total this will generate 25 (expected results) * 15 iterated samples = 375 expected synthetic rows (if model is able to follow the prompt!)\n",
    "N = 15\n",
    "\n",
    "PROMPT_METHODS = list(prompt_folders.keys()) # Select key from prompt_folders dict -> e.g. 'chain_of_thought'\n",
    "\n",
    "# When adding new providers, make sure the provider is added also and present in the api_handler.py script\n",
    "# All ollama models use : for seperator, which you need to specify here, however in all further file creation it will be replaced by a -\n",
    "MODEL = [\n",
    "    {\"name\":\"claude-3-opus-20240229\", \"provider\":\"anthropic\"},   #0\n",
    "    {\"name\":\"claude-3-sonnet-20240229\", \"provider\":\"anthropic\"}, #1\n",
    "    {\"name\":\"gemma:7b\", \"provider\":\"ollama\"},                    #2\n",
    "    {\"name\":\"gpt-3.5-turbo-0125\", \"provider\":\"openai\"},          #3\n",
    "    {\"name\":\"gpt-4-turbo\", \"provider\":\"openai\"},                 #4\n",
    "    {\"name\":\"gpt-4o\", \"provider\":\"openai\"},                      #5\n",
    "    {\"name\":\"llama2:13b\", \"provider\":\"ollama\"},                  #6\n",
    "    {\"name\":\"llama3:8b\", \"provider\":\"ollama\"},                   #7\n",
    "    {\"name\":\"llama3:70b\", \"provider\":\"ollama\"},                  #8\n",
    "    {\"name\":\"mistral:7b\", \"provider\":\"ollama\"},                  #9\n",
    "    {\"name\":\"mixtral:8x22b\", \"provider\":\"ollama\"},               #10\n",
    "    {\"name\":\"phi3:medium-128k\", \"provider\":\"ollama\"},            #11\n",
    "    {\"name\":\"phi3:mini-128k\", \"provider\":\"ollama\"},              #12\n",
    "][8] # Select model using index\n",
    "\n",
    "# These statistical text summaries were generated from the relevant dataset using R and turned into markdown format for prompt insertion.\n",
    "# Similar to the ORG_FOLDER only the d_clean_remove_small_samples_stats was used. Further research can see if including even more detailed description of each target_material will improve the overall characteristics of the synthetic data.\n",
    "# The cost and time will main constraint to not try out all these different combinations. However one should not refrain from looking into these files and experimenting with their own variations of it.\n",
    "STATS = [\n",
    "    'd_clean_remove_small_samples_stats',                       #0\n",
    "    'd_clean_remove_small_samples_stats-target_material',       #1\n",
    "    'd_clean_stats',                                            #2\n",
    "    'd_clean_stats-target_material',                            #3\n",
    "][0] # Select a specific statistics fill which will be inserted into the prompt template\n",
    "\n",
    "# Source folder of original nshot example rows that were sampled from the original data. For the research only the d_clean_remove_small_samples_ipr was used which was deemed to be most relevant.\n",
    "ORG_FOLDER = [\n",
    "    'd_clean_remove_small_samples_ipr', #0\n",
    "    'd_clean_remove_small_samples_iqr', #1\n",
    "    'd_full_clean_ipr',                 #2\n",
    "    'd_full_clean_ipr',                 #3\n",
    "][0] # Select the org folder you want to use with the index\n",
    "\n",
    "# The sampled filenames .csv of the selected source folder - These will be the files located that were specified as the ORG_FOLDER.\n",
    "ORG_SAMPLE_FILES = [\n",
    "    'rs_size_5',    #0\n",
    "    'rs_size_10',   #1\n",
    "    'rs_size_25',   #2\n",
    "    'rs_size_50',   #3\n",
    "    'rs_size_100',  #4\n",
    "    'rs_size_150',  #5\n",
    "] # The script will automatically iterate over these sample sizes (add extra if you need in the dedicated folder)\n",
    "\n",
    "#################################################################################################\n",
    "################################       END SELECT    ############################################\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prompt\n",
    "with open(f\"./../2_prompt_engineering/{PROMPT_TEMPLATE}\",\"r\") as f:\n",
    "    prompts_json = json.loads(f.read())\n",
    "\n",
    "# Count existing files for combination model, sample_size, prompt_method..\n",
    "def count_existing_files(directory, sample_size):\n",
    "    \"\"\"Count files in a directory that exactly match a specified sample size using string split.\"\"\"\n",
    "    count = 0\n",
    "    sample_size_tag = sample_size  # Create the tag to look for in the filename parts\n",
    "    for name in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, name)):\n",
    "            parts = name.split('+')\n",
    "            if sample_size_tag in parts:  # Check if the sample size tag is in the list of parts\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Function to recursively convert an object to a dictionary\n",
    "# This function is used to transform objects that are part of the different API responses (ollama, claude, openai) into a dict that is added as metadata to the model outputs for further inspection if needed.\n",
    "def convert_to_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_dict(v) for k, v in obj.items()}\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {k: convert_to_dict(v) for k, v in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_dict(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# A fix for saving filenames using model names of ollama models. Ubuntu 24 did not have a problem, but other earlier OS version might not allow : in the filenames.\n",
    "model_name_fix = MODEL['name'].replace(':','-')\n",
    "\n",
    "# In the following loop we are iterating over the different sample files (5,10,25,50,100,150) that correspond to the N-Shot examples that are included with the prompt.\n",
    "# Then in the next inner loop we iterate over every prompt method and use this as template where all relevant fields are inserted into e.g.: [INSERT EXAMPLE HERE] refering to the N-Shot Examples and [INSERT STATS HERE] refering to the descriptive statistics.\n",
    "for ORG_SAMPLE_FILE in ORG_SAMPLE_FILES:\n",
    "    for PROMPT_METHOD in PROMPT_METHODS:\n",
    "        # Define the directory path where the generate samples will be stored. \n",
    "        # This will be ./3_llm_generation/outputs/gpt-4o/cot for example.\n",
    "        directory_path = f\"./../3_llm_generation/outputs/{model_name_fix}/{prompt_folders[PROMPT_METHOD]}\"\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "        # The files that already exist for the model, sample_sizes and prompt_method will be subtracted from the N count specified above (e.g. 15 required responses).\n",
    "        # For example if we rerun the script, we will not unnecessarily regenerated files beyond 15 responses. This will ensure that the total amount of responses for each model is equal.\n",
    "        existing_files = count_existing_files(directory_path,ORG_SAMPLE_FILE)\n",
    "        samples_to_generate = max(0, N - existing_files)\n",
    "\n",
    "        # Already got N samples, can skip generation!\n",
    "        if samples_to_generate == 0:\n",
    "            print(f\"Already have {N} files for {PROMPT_METHOD} with sample size {ORG_SAMPLE_FILE}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # If we don't have enough generated samples for the model, prompt_method & sample_size combination then generate more!\n",
    "        # Print out how many are left to generate.\n",
    "        print(f\"Need to generate {samples_to_generate} more files for {PROMPT_METHOD} with sample size {ORG_SAMPLE_FILE}.\")\n",
    "\n",
    "        # Load the system and user prompt from the current prompt_method as variables\n",
    "        system_prompt = prompts_json[PROMPT_METHOD]['system']\n",
    "        user_prompt = prompts_json[PROMPT_METHOD]['user']\n",
    "\n",
    "        # Define the prompts in a structured JSON format as how the current api_handler expects it. In the api_handler the information required by the providers api endpoint is transformed into their required format.\n",
    "        # These are still the raw prompt text, where the [INSERT EXAMPLE HERE] & [INSERT STATS HERE] have to be replaced.\n",
    "        prompts = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":user_prompt\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Read the CSV data for the N-Shot Examples, e.g. ./1_sample_preparation/org_samples/d_clean_remove_small_samples_ipr/rs_size_100.csv\"\n",
    "        with open(f\"./../1_sample_preparation/org_samples/{ORG_FOLDER}/{ORG_SAMPLE_FILE}.csv\", \"r\") as f:\n",
    "            csv_data = f.read()\n",
    "\n",
    "        # Read the CSV data for the statistical description, e.g. ./2_prompt_engineering/d_clean_remove_small_samples_stats\" if the stats file doesn't exist replace it with an empty string.\n",
    "        if STATS != '':\n",
    "            with open(f\"./../2_prompt_engineering/{STATS}\", \"r\") as f:\n",
    "                stats_data = f.read()\n",
    "        else:\n",
    "            stats_data = ''\n",
    "\n",
    "        # Read the N-Shot Examples as CSV data into a DataFrame\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "        # Ensure the intensity column is numeric to handle scientific notation\n",
    "        df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce')\n",
    "\n",
    "        # Convert each row to the desired format, some numbers are very long and they cost high amount of tokens. To reduce the token cost the row of intensity if reduced to 3 signficant digits.\n",
    "        # Example:\n",
    "        # target_material,target_thickness,pulse_width,energy,spot_size,intensity,power,cutoff_energy\n",
    "        # aluminium,3.0,180,10.9,9.7,56830000000000000000,60560000000000,16.0\n",
    "        # gold,4.0,2280,46.737,2.87,219700000000000000000,20500000000000,17.6\n",
    "        # The value of 219700000000000000000 is converted to 2.197E20\n",
    "        formatted_data = df.apply(\n",
    "            lambda row: f\"['{row['target_material']}', {row['target_thickness']}, {row['pulse_width']}, {row['energy']}, {row['spot_size']}, {row['intensity']:.3E}, {row['power']}, {row['cutoff_energy']}]\", \n",
    "            axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        # Combine the formatted list of rows into a single string\n",
    "        example_data = \"\\n\".join(formatted_data)\n",
    "\n",
    "        # Replace the N-SHOT placeholder, [INSERT EXAMPLE HERE], with actual examples data in user prompt\n",
    "        prompts[1]['content'] = prompts[1]['content'].replace(\"[INSERT EXAMPLE HERE]\", example_data)\n",
    "\n",
    "        # Replace the statistics placeholder, [INSERT STATS HERE], with actual statistics data in user prompt\n",
    "        prompts[1]['content'] = prompts[1]['content'].replace(\"[INSERT STATS HERE]\", stats_data)\n",
    "\n",
    "        # Display the final structured prompts\n",
    "        print('\\n[PROMPT INPUTS]:\\n',prompts)\n",
    "\n",
    "        # Print the summary stats\n",
    "        print('\\n[SUMMARY STATS]:\\n',stats_data)\n",
    "\n",
    "        # Print the result\n",
    "        print('\\n[N-SHOT SAMPLES]:\\n',example_data,'\\n\\n')\n",
    "\n",
    "        # In case we get an error, retry and keep track of how many successfull API responses that were already received.\n",
    "        successful_attempts = 0\n",
    "        retry_count = 0\n",
    "        while successful_attempts != samples_to_generate:\n",
    "            try:\n",
    "                ######################################################\n",
    "                ####### Actual API Requests send and received ########\n",
    "                ######################################################\n",
    "                print(f\"Starting generation {successful_attempts+1}/{samples_to_generate} with {PROMPT_METHOD} for model {model_name_fix} using {ORG_FOLDER}/{ORG_SAMPLE_FILE}.csv ...\")\n",
    "                \n",
    "                # Get current time for appending to file_name's\n",
    "                TIMESTAMP = datetime.now().isoformat()[:-7].replace(':','-')\n",
    "\n",
    "                ##############################################################################################\n",
    "                ####### Currently supported API Providers                                          ###########\n",
    "                #######   - OLLAMA    (Locally downlaoded models, ollama server must be running!)  ###########\n",
    "                #######   - OPENAI    (GPT-4o, GPT-3.5, etc.)                                      ###########\n",
    "                #######   - ANTHROPIC (Claude, etc.)                                               ###########\n",
    "                ##############################################################################################\n",
    "\n",
    "                # Here Ollama will use the model name without fix  -> llama3:70b instead of llama3-70b (due to file systems not allowing : on some OS's)\n",
    "                if(MODEL['provider'] == 'ollama'):\n",
    "                    # Make API request to ollama local server - Make sure the models are downloaded and you can prompt them as usual.\n",
    "                    completion =  ollama_req(model=MODEL['name'],messages=prompts)\n",
    "                elif(MODEL['provider'] == 'openai'):\n",
    "                    # Make API request to OpenAI API - Specifiy API KEY in .env!\n",
    "                    completion = openai_req(model=MODEL['name'],messages=prompts)\n",
    "                elif(MODEL['provider'] == 'anthropic'):\n",
    "                    # Make API request to Anthropic API - Specifiy API KEY in .env!\n",
    "                    completion = anthropic_req(model=MODEL['name'],max_tokens=4096,messages=prompts)\n",
    "\n",
    "                ### For debugging if you want to see the whole API response\n",
    "                # print(completion)\n",
    "\n",
    "                #### The content is the text response of the large language model of interest.\n",
    "                print('[RESPONSE]:\\n', completion['content'])\n",
    "                \n",
    "                ### Turn the entire completion into a dictionary that we can add into the metadata.\n",
    "                ### The content is seperated for easy access, for more info on how these outputs are structured in final form see the /outputs folder\n",
    "                ### Generation metadata is saved to keep track of how the synthethic data was generated using which files.\n",
    "                dict_completion = convert_to_dict(completion)\n",
    "                dict_completion['inputs'] = prompts\n",
    "                dict_completion['metadata_gen'] = {\n",
    "                    'N': samples_to_generate,\n",
    "                    'MODEL': model_name_fix,\n",
    "                    'ORG_FOLDER': ORG_FOLDER,\n",
    "                    'ORG_SAMPLE_FILE': ORG_SAMPLE_FILE,\n",
    "                    'PROMPT_METHOD': PROMPT_METHOD,\n",
    "                    'PROMPT_TEMPLATE': PROMPT_TEMPLATE,\n",
    "                    'STATS': STATS,\n",
    "                }\n",
    "\n",
    "                # Define the file path for saving the generated sample JSON file. e.g. ./outputs/gpt-4o/cov/gpt-4o+1prompt+d_clean_remove_small_samples_ipr+rs_size_5+cot+2024-06-10T20-07-00.json\n",
    "                # The use of '+' makes it easier to split the filename by relevant information, e.g. model_name is index 0 after splitting, and sample_size (rs_size_5) is index -2 (counting backwards in list).\n",
    "                file_path = f\"{directory_path}/{model_name_fix}+{PROMPT_TEMPLATE.split('_')[0]}+{ORG_FOLDER}+{ORG_SAMPLE_FILE}+{prompt_folders[PROMPT_METHOD]}+{TIMESTAMP}.json\"\n",
    "\n",
    "                # Write out the file\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(json.dumps(dict_completion, indent=4))\n",
    "\n",
    "                ## Increment the amount of successfull generations for combination; model, prompt_method and sample_size.\n",
    "                successful_attempts += 1\n",
    "\n",
    "            ### Error handling, in case we fail to generate. Print out the traceback for debug and increase the retry count with sleep of 2 seconds.\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                traceback.print_exc()\n",
    "                retry_count += 1\n",
    "                print(f\"Retrying... ({retry_count})\")\n",
    "                time.sleep(2)  # Optional: add a delay before retrying\n",
    "        \n",
    "        ### After the whole model generation is complete the final message is printed out.\n",
    "        ### Adviced is to run the current cell once more, it will quickly check if all the samples are successfull or if any is missing from the target N = 15.\n",
    "        print(f\"Finished generating files for {PROMPT_METHOD} with sample size {ORG_SAMPLE_FILE}.\")\n",
    "        print(f\"************* Finished {successful_attempts}/{samples_to_generate} generations! *************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
