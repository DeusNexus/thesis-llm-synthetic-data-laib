{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import traceback\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "from api_handler import ollama_req, openai_req, anthropic_req\n",
    "\n",
    "# Shorthand dict\n",
    "prompt_folders = {\n",
    "    'chain_of_thought':'cot',                       #0\n",
    "    'skeleton_of_thought':'sot',                    #1\n",
    "    'self_consistency':'sc',                        #2\n",
    "    'generated_knowledge':'gk',                     #3\n",
    "    'least_to_most':'ltm',                          #4\n",
    "    'chain_of_verification':'cov',                  #5\n",
    "    'step_back_prompting':'sbp',                    #6\n",
    "    'rephrase_and_respond':'rar',                   #7\n",
    "    'emotion_prompt':'em',                          #8\n",
    "    'directional_stimuli':'ds',                     #9\n",
    "    'recursive_criticism_and_improvement':'rcai',   #10\n",
    "    'reverse_prompting':'rp'                        #11\n",
    "}\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "\n",
    "# Run generation of synthetic data prompts N times.\n",
    "N = 15\n",
    "\n",
    "PROMPT_METHODS = list(prompt_folders.keys()) # Select key from prompt_folders dict -> e.g. 'chain_of_thought'\n",
    "PROMPT_TEMPLATE = '1prompt_templates_system_stats.json'\n",
    "\n",
    "# When adding new providers, make sure the provider is added also and present in the api_handler.py script\n",
    "# All ollama models use : for seperator, which you need to specify here, however in all further file creation it will be replaced by a -\n",
    "MODEL = [\n",
    "    {\"name\":\"claude-3-opus-20240229\", \"provider\":\"anthropic\"},   #0\n",
    "    {\"name\":\"claude-3-sonnet-20240229\", \"provider\":\"anthropic\"}, #1 DONE\n",
    "    {\"name\":\"falcon:40b\", \"provider\":\"ollama\"},                  #2\n",
    "    {\"name\":\"falcon:180b\", \"provider\":\"ollama\"},                 #3\n",
    "    {\"name\":\"gemma:7b\", \"provider\":\"ollama\"},                    #4\n",
    "    {\"name\":\"gpt-3.5-turbo-0125\", \"provider\":\"openai\"},          #5 DONE\n",
    "    {\"name\":\"gpt-4-turbo\", \"provider\":\"openai\"},                 #6 DONE\n",
    "    {\"name\":\"gpt-4o\", \"provider\":\"openai\"},                      #7\n",
    "    {\"name\":\"llama2:13b\", \"provider\":\"ollama\"},                  #8\n",
    "    {\"name\":\"llama3:8b\", \"provider\":\"ollama\"},                   #9\n",
    "    {\"name\":\"llama3:70b\", \"provider\":\"ollama\"},                  #10\n",
    "    {\"name\":\"mistral:7b\", \"provider\":\"ollama\"},                  #11\n",
    "    {\"name\":\"mixtral:8x22b\", \"provider\":\"ollama\"},               #12\n",
    "    {\"name\":\"phi3:medium-128k\", \"provider\":\"ollama\"},            #13\n",
    "    {\"name\":\"phi3:mini-128k\", \"provider\":\"ollama\"},              #14\n",
    "    {\"name\":\"qwen2:72b\", \"provider\":\"ollama\"},                   #15\n",
    "][8]\n",
    "\n",
    "# Source folder of original samples\n",
    "ORG_FOLDER = [\n",
    "    'd_clean_remove_small_samples_ipr', #0\n",
    "    'd_clean_remove_small_samples_iqr', #1\n",
    "    'd_full_clean_ipr',                 #2\n",
    "    'd_full_clean_ipr',                 #3\n",
    "][0] # Selected num\n",
    "\n",
    "# The sampled filenames .csv of the selected source folder\n",
    "ORG_SAMPLE_FILES = [\n",
    "    'rs_size_5',    #0\n",
    "    'rs_size_10',   #1\n",
    "    'rs_size_25',   #2\n",
    "    'rs_size_50',   #3\n",
    "    'rs_size_100',  #4\n",
    "    'rs_size_150',  #5\n",
    "    # 'rs_size_250',  #6\n",
    "] # Selected num\n",
    "\n",
    "# These statistical text summaries were generated from the relevant dataset using R and turned into markdown format for prompt insertion.\n",
    "STATS = [\n",
    "    'd_clean_remove_small_samples_stats',                       #0\n",
    "    'd_clean_remove_small_samples_stats-target_material',       #1\n",
    "    'd_clean_stats',                                            #2\n",
    "    'd_clean_stats-target_material',                            #3\n",
    "][0] # Selected num\n",
    "\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "\n",
    "# Load the prompt\n",
    "with open(f\"./../2_prompt_engineering/{PROMPT_TEMPLATE}\",\"r\") as f:\n",
    "    prompts_json = json.loads(f.read())\n",
    "\n",
    "# Count existing files for combination model, sample_size, prompt_method..\n",
    "def count_existing_files(directory, sample_size):\n",
    "    \"\"\"Count files in a directory that exactly match a specified sample size using string split.\"\"\"\n",
    "    count = 0\n",
    "    sample_size_tag = sample_size  # Create the tag to look for in the filename parts\n",
    "    for name in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, name)):\n",
    "            parts = name.split('+')\n",
    "            if sample_size_tag in parts:  # Check if the sample size tag is in the list of parts\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Function to recursively convert an object to a dictionary\n",
    "def convert_to_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_dict(v) for k, v in obj.items()}\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {k: convert_to_dict(v) for k, v in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_dict(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "model_name_fix = MODEL['name'].replace(':','-')\n",
    "for ORG_SAMPLE_FILE in ORG_SAMPLE_FILES:\n",
    "    for PROMPT_METHOD in PROMPT_METHODS:\n",
    "        # Define the directory path\n",
    "        directory_path = f\"./../3_llm_generation/outputs/{model_name_fix}/{prompt_folders[PROMPT_METHOD]}\"\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "        existing_files = count_existing_files(directory_path,ORG_SAMPLE_FILE)\n",
    "        samples_to_generate = max(0, N - existing_files)\n",
    "\n",
    "        # Already got N samples, can skip generation!\n",
    "        if samples_to_generate == 0:\n",
    "            print(f\"Already have {N} files for {PROMPT_METHOD} with sample size {ORG_SAMPLE_FILE}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Rest of your code for data preparation and generation goes here\n",
    "        print(f\"Need to generate {samples_to_generate} more files for {PROMPT_METHOD} with sample size {ORG_SAMPLE_FILE}.\")\n",
    "\n",
    "        system_prompt = prompts_json[PROMPT_METHOD]['system']\n",
    "        user_prompt = prompts_json[PROMPT_METHOD]['user']\n",
    "\n",
    "        # Define the prompts in a structured JSON format\n",
    "        prompts = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":user_prompt\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Read the CSV data from the file\n",
    "        with open(f\"./../1_sample_preperation/org_samples/{ORG_FOLDER}/{ORG_SAMPLE_FILE}.csv\", \"r\") as f:\n",
    "            csv_data = f.read()\n",
    "\n",
    "        # Read the CSV data from the file\n",
    "        if STATS != '':\n",
    "            with open(f\"./../2_prompt_engineering/{STATS}\", \"r\") as f:\n",
    "                stats_data = f.read()\n",
    "        else:\n",
    "            stats_data = ''\n",
    "\n",
    "        # Read the CSV data into a DataFrame\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "        # Ensure the intensity column is numeric to handle scientific notation\n",
    "        df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce')\n",
    "\n",
    "        # Convert each row to the desired format\n",
    "        formatted_data = df.apply(\n",
    "            lambda row: f\"['{row['target_material']}', {row['target_thickness']}, {row['pulse_width']}, {row['energy']}, {row['spot_size']}, {row['intensity']:.3E}, {row['power']}, {row['cutoff_energy']}]\", \n",
    "            axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        # Combine the formatted rows into a single string\n",
    "        example_data = \"\\n\".join(formatted_data)\n",
    "\n",
    "        # Replace placeholder with actual example data in user prompt\n",
    "        prompts[1]['content'] = prompts[1]['content'].replace(\"[INSERT EXAMPLE HERE]\", example_data)\n",
    "\n",
    "        # Replace placeholder with actual example data in user prompt\n",
    "        prompts[1]['content'] = prompts[1]['content'].replace(\"[INSERT STATS HERE]\", stats_data)\n",
    "\n",
    "        # Display the final structured prompts\n",
    "        print('\\n[PROMPT INPUTS]:\\n',prompts)\n",
    "\n",
    "        # Print the summary stats\n",
    "        print('\\n[SUMMARY STATS]:\\n',stats_data)\n",
    "\n",
    "        # Print the result\n",
    "        print('\\n[N-SHOT SAMPLES]:\\n',example_data,'\\n\\n')\n",
    "\n",
    "        successful_attempts = 0\n",
    "        retry_count = 0\n",
    "        while successful_attempts != samples_to_generate:\n",
    "            try:\n",
    "                print(f\"Starting generation {successful_attempts+1}/{samples_to_generate} with {PROMPT_METHOD} for model {model_name_fix} using {ORG_FOLDER}/{ORG_SAMPLE_FILE}.csv ...\")\n",
    "                # Get current time\n",
    "                TIMESTAMP = datetime.now().isoformat()[:-7].replace(':','-')\n",
    "\n",
    "                # Here Ollama will use the model name without fix  -> llama3:70b instead of llama3-70b\n",
    "                if(MODEL['provider'] == 'ollama'):\n",
    "                    # Make API request to ollama local server - Make sure the models are downloaded and you can prompt them as usual.\n",
    "                    completion =  ollama_req(model=MODEL['name'],messages=prompts)\n",
    "                elif(MODEL['provider'] == 'openai'):\n",
    "                    # Make API request to OpenAI API - Specifiy API KEY in .env!\n",
    "                    completion = openai_req(model=MODEL['name'],messages=prompts)\n",
    "                elif(MODEL['provider'] == 'anthropic'):\n",
    "                    # Make API request to Anthropic API - Specifiy API KEY in .env!\n",
    "                    completion = anthropic_req(model=MODEL['name'],max_tokens=4096,messages=prompts)\n",
    "\n",
    "                # print(completion)\n",
    "\n",
    "                print('[RESPONSE]:\\n', completion['content'])\n",
    "                    \n",
    "                dict_completion = convert_to_dict(completion)\n",
    "                dict_completion['inputs'] = prompts\n",
    "                dict_completion['metadata_gen'] = {\n",
    "                    'N': samples_to_generate,\n",
    "                    'MODEL': model_name_fix,\n",
    "                    'ORG_FOLDER': ORG_FOLDER,\n",
    "                    'ORG_SAMPLE_FILE': ORG_SAMPLE_FILE,\n",
    "                    'PROMPT_METHOD': PROMPT_METHOD,\n",
    "                    'PROMPT_TEMPLATE': PROMPT_TEMPLATE,\n",
    "                    'STATS': STATS,\n",
    "                }\n",
    "\n",
    "                # Define the file path\n",
    "                file_path = f\"{directory_path}/{model_name_fix}+{PROMPT_TEMPLATE.split('_')[0]}+{ORG_FOLDER}+{ORG_SAMPLE_FILE}+{prompt_folders[PROMPT_METHOD]}+{TIMESTAMP}.json\"\n",
    "\n",
    "                # Write to the file\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(json.dumps(dict_completion, indent=4))\n",
    "\n",
    "                successful_attempts += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                traceback.print_exc()\n",
    "                retry_count += 1\n",
    "                print(f\"Retrying... ({retry_count})\")\n",
    "                time.sleep(2)  # Optional: add a delay before retrying\n",
    "        \n",
    "        print(f\"Finished generating files for {PROMPT_METHOD} with sample size {ORG_SAMPLE_FILE}.\")\n",
    "        print(f\"************* Finished {successful_attempts}/{samples_to_generate} generations! *************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
