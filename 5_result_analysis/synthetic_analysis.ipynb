{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use ggplot\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "features = ['target_material','target_thickness','pulse_width','energy','spot_size','intensity','power','cutoff_energy']\n",
    "numeric_features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "# Load original dataset that was used to generate samples\n",
    "df_original = pd.read_csv('../1_sample_preparation/source/d_clean_remove_small_samples.csv')\n",
    "# Make sure required features are numeric\n",
    "df_original[numeric_features] = df_original[numeric_features].astype(float)\n",
    "df_original.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Load synthetic data set\n",
    "df_synthetic = pd.read_csv('../4_response_extraction/synthetic_data_rows.csv')\n",
    "# Drop column\n",
    "df_synthetic.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# Make sure required features are numeric\n",
    "df_synthetic[numeric_features] = df_synthetic[numeric_features].astype(float)\n",
    "df_synthetic.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_synth_raw = df_synthetic.copy()\n",
    "\n",
    "# Function to remove outliers for a specific feature\n",
    "def remove_outliers(df, feature, lower_percentile, upper_percentile):\n",
    "    Q1 = df[feature].quantile(lower_percentile)\n",
    "    Q3 = df[feature].quantile(upper_percentile)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "\n",
    "# Remove outliers for all numeric features\n",
    "for nf in numeric_features:\n",
    "    df_synth_raw = remove_outliers(df_synth_raw, nf, 0.005, 0.995)\n",
    "\n",
    "print(f\"Original DataFrame length: {len(df_synthetic)}\")\n",
    "print(f\"DataFrame length after removing outliers 'RAW' for Inf values: {len(df_synth_raw)} - Rows lost: {len(df_synthetic) - len(df_synth_raw)}\")\n",
    "\n",
    "print(\"\\ndf_original head:\",df_original.head())\n",
    "print(\"\\ndf_original info:\",df_original.info())\n",
    "print(\"\\ndf_original unique prompt_method: NA\")\n",
    "print(\"\\ndf_original unique prompt_short: NA\")\n",
    "print(\"\\ndf_original unique sample_size: NA\")\n",
    "print(\"\\ndf_original unique target_material:\",df_original['target_material'].unique())\n",
    "print(\"\\ndf_original unique model: NA\")\n",
    "\n",
    "print(\"\\ndf_synth_raw head:\",df_synth_raw.head())\n",
    "print(\"\\ndf_synth_raw info:\",df_synth_raw.info())\n",
    "print(\"\\ndf_synth_raw unique prompt_method:\",df_synth_raw['prompt_method'].unique())\n",
    "print(\"\\ndf_synth_raw unique prompt_short:\",df_synth_raw['prompt_short'].unique())\n",
    "print(\"\\ndf_synth_raw unique sample_size:\",df_synth_raw['sample_size'].unique())\n",
    "print(\"\\ndf_synth_raw unique target_material:\",df_synth_raw['target_material'].unique())\n",
    "print(\"\\ndf_synth_raw unique model:\",df_synth_raw['model'].unique())\n",
    "\n",
    "df_synthetic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each 'target_material'\n",
    "material_counts = df_synth_raw['target_material'].value_counts()\n",
    "\n",
    "print('Unique Materials:',len(df_synth_raw['target_material'].unique()))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width = 800, height = 400, background_color ='white')\n",
    "\n",
    "# Generate a word cloud using the frequencies\n",
    "wordcloud.generate_from_frequencies(material_counts.to_dict())\n",
    "\n",
    "# Display the WordCloud image:\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.title('Wordcloud of frequent target_material (all models)',y=1.05)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./images/word_cloud_target_material.jpg',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each 'target_material'\n",
    "material_counts = df_synth_raw['target_material'].value_counts()\n",
    "\n",
    "# Assuming 'material_counts' holds the value counts of 'target_material' from your DataFrame\n",
    "top_25_materials = material_counts.head(50)  # Get the top 50 materials for better illustration\n",
    "\n",
    "# Create a DataFrame from the top 25 materials for easier plotting\n",
    "top_25_df = pd.DataFrame(top_25_materials).reset_index()\n",
    "top_25_df.columns = ['target_material', 'count']\n",
    "\n",
    "# Calculate cumulative percentage\n",
    "total_rows = df_synthetic.shape[0]  # Total number of rows in the original DataFrame\n",
    "top_25_df['cumulative_percent'] = top_25_df['count'].cumsum() / total_rows * 100\n",
    "\n",
    "# Initialize the matplotlib figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)  # Share y-axis across subplots\n",
    "\n",
    "# Left subplot for frequency\n",
    "sns.barplot(x='count', y='target_material', data=top_25_df, ax=axes[0], hue='target_material',palette='viridis')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Frequency (Log Scale)')\n",
    "axes[0].set_ylabel('Material')\n",
    "axes[0].set_title('Top 50 Frequent Materials')\n",
    "\n",
    "# Right subplot for cumulative percentage\n",
    "sns.lineplot(x='cumulative_percent', y='target_material', data=top_25_df, ax=axes[1],color='black')\n",
    "axes[1].set_xlabel('Cumulative Percentage (%)')\n",
    "axes[1].set_title('Total Share of Rows by Material')\n",
    "\n",
    "# Set x-axis to have a tick every 5 percent\n",
    "axes[1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "# Optionally, you can format the ticks to show them as percentages\n",
    "axes[1].xaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# If you want to limit the x-axis to show up to 100% only\n",
    "axes[1].set_xlim(40, 100)\n",
    "\n",
    "# Annotate each point on the line plot\n",
    "for index, row in top_25_df.iterrows():\n",
    "    axes[1].annotate(f\"{row['cumulative_percent']:.1f}%\", \n",
    "                     (row['cumulative_percent'], row['target_material']), \n",
    "                     textcoords=\"offset points\", \n",
    "                     xytext=(-14,-10), \n",
    "                     ha='center', \n",
    "                     color='black', \n",
    "                     size=7,\n",
    "    )\n",
    "# Plotting the points as black dots\n",
    "axes[1].scatter(top_25_df['cumulative_percent'], top_25_df['target_material'], color='black', s=20)  # s is the size of the dot\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('./images/top_50_materials_comparison.jpg', dpi=300)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "num_models = len(df_synth_raw['model'].unique())\n",
    "num_features = len(numeric_features)\n",
    "\n",
    "# Calculate the optimal figure size\n",
    "fig_width = min(20, 2.5 * num_features)  # Cap the width at 20 inches\n",
    "fig_height = min(30, 2 * num_models)  # Cap the height at 30 inches\n",
    "\n",
    "# Initialize the matplotlib figure with subplots\n",
    "fig, axes = plt.subplots(num_models, num_features, figsize=(fig_width, fig_height))\n",
    "\n",
    "# If there's only one model, wrap axes in a list to make it 2D\n",
    "if num_models == 1:\n",
    "    axes = np.array([axes])\n",
    "\n",
    "for i, model_name in enumerate(df_synth_raw['model'].unique()):\n",
    "    # Filter the DataFrame for the specific model\n",
    "    model_df = df_synth_raw[df_synth_raw['model'] == model_name].copy()\n",
    "\n",
    "    for j, feature in enumerate(numeric_features):\n",
    "        # Create a boxplot for the numeric features of this specific model\n",
    "        sns.boxplot(data=model_df, y=feature, ax=axes[i, j], orient='v', color='red', fliersize=2)\n",
    "        \n",
    "        # Set the xlabel on each subplot\n",
    "        axes[i, j].set_xlabel('')\n",
    "        \n",
    "        # Only set the feature name for the bottom row\n",
    "        if i == num_models - 1:\n",
    "            axes[i, j].set_xlabel(feature, fontsize=10, fontweight='normal')\n",
    "        \n",
    "        # Remove y-label for all subplots\n",
    "        axes[i, j].set_ylabel('')\n",
    "\n",
    "        # Rotate x-axis labels for better readability\n",
    "        axes[i, j].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        axes[i, j].tick_params(axis='y', labelsize=8)\n",
    "\n",
    "        if i % 2 != 0:\n",
    "            axes[i, j].patch.set_facecolor((0.95, 0.95, 0.95))  # Light grey color\n",
    "        else:\n",
    "            axes[i, j].patch.set_facecolor((0.99, 0.99, 0.99))  # Light grey color\n",
    "\n",
    "    # Add model name to the top left corner of each row\n",
    "    axes[i, 0].text(-0.3, 1.125, model_name, transform=axes[i, 0].transAxes, \n",
    "                    fontsize=12, fontweight='light', ha='left', va='bottom')\n",
    "\n",
    "    # Extend the shaded background to include the model name area for shaded rows\n",
    "    if i % 2 == 0:\n",
    "        axes[i, 0].add_patch(plt.Rectangle((-0.35, -0.1), 10, 1.4, \n",
    "                                           fill=True, transform=axes[i, 0].transAxes, \n",
    "                                           facecolor=(0.96, 0.96, 0.96), clip_on=False, zorder=0))\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.435, top=0.935, left=0.15)\n",
    "\n",
    "# Add a main title\n",
    "fig.suptitle('Feature Distribution Across Models - Raw', fontsize=16, fontweight='bold', y=0.975)\n",
    "fig.text(s='Raw data which might include extreme outliers due to mistakes large language models can introduce.',y=0.957,x=0.255,fontsize=12, fontweight='light')\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.savefig('./images/boxplot_outliers_models_raw.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_kde_plot(data, title, subtitle):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    synthetic_models = [model for model in data['model'].unique() if model != 'original data']\n",
    "    color_palette = sns.color_palette(\"husl\", n_colors=len(synthetic_models))\n",
    "\n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Determine the x-axis range\n",
    "        x_min = max(0, data[feature].quantile(0.0025))  # Use 1st percentile, but never go below 0\n",
    "        x_max = data[feature].quantile(0.9975)  # Use 99th percentile to exclude extreme outliers\n",
    "        x_range = x_max - x_min\n",
    "        x_padding = x_range * 0.1  # Add 10% padding on each side\n",
    "        \n",
    "        original_data = data[data['model'] == 'original data']\n",
    "        if not original_data.empty:\n",
    "            sns.kdeplot(data=original_data, x=feature, ax=ax, color='black', alpha=1, label='Original Data', bw_adjust=1.5)\n",
    "        \n",
    "        for j, model in enumerate(synthetic_models):\n",
    "            model_data = data[data['model'] == model]\n",
    "\n",
    "            if model_data[feature].nunique() == 1:\n",
    "                print(f\"  Skipping plot for {model} - all values are the same\")\n",
    "                continue\n",
    "            \n",
    "            sns.kdeplot(data=model_data, x=feature, ax=ax, color=color_palette[j], alpha=0.7, label=model, bw_adjust=1.5)\n",
    "        \n",
    "        ax.set_title(f\"log({feature})\", fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Density', fontsize=12)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.set_xlim(x_min - x_padding, x_max + x_padding)\n",
    "        ax.legend().remove()\n",
    "        \n",
    "        # Remove scientific notation from x-axis\n",
    "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[-1].legend(handles, labels, title='Models', title_fontsize='15', fontsize='13', loc='center', bbox_to_anchor=(0.5, 0.5), bbox_transform=axes[-1].transAxes)\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(title, fontsize=20, fontweight='bold', y=1.02)\n",
    "    fig.text(0.5, 0.97, subtitle, ha='center', fontsize=14, fontstyle='italic')\n",
    "    plt.subplots_adjust(top=0.9, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    return fig\n",
    "\n",
    "df_original_log = df_original.copy()\n",
    "df_synth_raw_log = df_synth_raw.copy()\n",
    "\n",
    "# Add 'model' column to df_original and select only numeric features\n",
    "df_original_log['model'] = 'original data'\n",
    "df_original_log = df_original_log[['model'] + numeric_features]\n",
    "\n",
    "# Log transform numeric features\n",
    "for feature in numeric_features:\n",
    "    df_original_log[feature]            = np.log1p(df_original_log[feature])\n",
    "    df_synth_raw_log[feature] = np.log1p(df_synth_raw[feature])\n",
    "\n",
    "# Add original data to cleaned dataframes\n",
    "df_clean_raw_with_original = pd.concat([df_synth_raw_log, df_original_log], ignore_index=True)\n",
    "\n",
    "# Create and save plots for raw data (without inf/NaN values), IQR-cleaned data, and IPR-cleaned data\n",
    "raw_plot = create_kde_plot(df_clean_raw_with_original, 'Kernel Density Estimation of Log-Transformed Features Across Models (Raw Data)', 'Extreme outliers removed (0.5th to 99.5th percentile)')\n",
    "raw_plot.savefig('./images/kde_features_across_models_raw_log.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_remove_outliers and df_original are already loaded\n",
    "\n",
    "# Selecting numerical features for PCA\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "# Ensure df_original has all columns from df_remove_outliers\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "# Add 'source' column\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "# Drop NaN values only from the synthetic data\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "# Combine the cleaned synthetic DataFrame with the original DataFrame\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "# Now proceed with PCA on the combined DataFrame\n",
    "X = combined_df[features].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "# Standardizing the features\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Proceed with PCA\n",
    "pca = PCA(n_components=2)  # Example: reducing to 2 principal components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# Adding the source and model information back to the PCA DataFrame\n",
    "pca_df = pd.concat([pca_df, combined_df.reset_index(drop=True)[['source', 'model']]], axis=1)\n",
    "\n",
    "# Handle NaNs in the 'model' column for original data\n",
    "pca_df['model'].fillna('original', inplace=True)\n",
    "\n",
    "# Separate the synthetic and original data points\n",
    "synthetic_points = pca_df[pca_df['source'] == 'synthetic']\n",
    "original_points = pca_df[pca_df['source'] == 'original']\n",
    "\n",
    "# Plotting the PCA result\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.xlim([-2,20])\n",
    "\n",
    "# Plot synthetic points first\n",
    "sns.scatterplot(data=synthetic_points, x='Principal Component 1', y='Principal Component 2', hue='model', style='model', palette='Set1', alpha=0.5, legend=True)\n",
    "\n",
    "# Plot original points on top in black\n",
    "sns.scatterplot(data=original_points, x='Principal Component 1', y='Principal Component 2', color='black')\n",
    "\n",
    "plt.title('PCA of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def create_scatter_pca(synthetic, original,synthetic_name):\n",
    "    # Selecting numerical features for PCA\n",
    "    features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "    # Ensure original has all columns from synthetic\n",
    "    missing_cols = set(synthetic.columns) - set(original.columns)\n",
    "    for col in missing_cols:\n",
    "        original[col] = np.nan\n",
    "\n",
    "    # Add 'source' column\n",
    "    synthetic['source'] = 'synthetic'\n",
    "    original['source'] = 'original'\n",
    "\n",
    "    # Drop NaN values only from the synthetic data\n",
    "    df_synth_raw_copy = synthetic.copy()\n",
    "    df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "    # Combine the cleaned synthetic DataFrame with the original DataFrame\n",
    "    combined_df = pd.concat([df_synth_raw_copy, original], ignore_index=True)\n",
    "\n",
    "    # Now proceed with PCA on the combined DataFrame\n",
    "    X = combined_df[features].copy()\n",
    "\n",
    "    # Standardizing the features\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Proceed with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Create a DataFrame with the principal components\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "    # Adding the source and model information back to the PCA DataFrame\n",
    "    pca_df = pd.concat([pca_df, combined_df.reset_index(drop=True)[['source', 'model']]], axis=1)\n",
    "\n",
    "    # Handle NaNs in the 'model' column for original data\n",
    "    pca_df['model'].fillna('original', inplace=True)\n",
    "\n",
    "    # Separate the synthetic and original data points\n",
    "    synthetic_points = pca_df[pca_df['source'] == 'synthetic']\n",
    "    original_points = pca_df[pca_df['source'] == 'original']\n",
    "\n",
    "    # Get unique models\n",
    "    unique_models = combined_df['model'].unique()\n",
    "    unique_models = [model for model in unique_models if isinstance(model, str)]\n",
    "\n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_models = len(unique_models)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes to iterate easily\n",
    "\n",
    "    for i, model_name in enumerate(unique_models):\n",
    "        ax = axes[i]\n",
    "        # Plot synthetic points first\n",
    "        sns.scatterplot(data=synthetic_points[synthetic_points['model'] == model_name], \n",
    "                        x='Principal Component 1', y='Principal Component 2', \n",
    "                        hue='model', style='model', palette='Set1', alpha=0.5, \n",
    "                        legend=False, ax=ax)\n",
    "\n",
    "        # Plot original points on top in black\n",
    "        sns.scatterplot(data=original_points, x='Principal Component 1', y='Principal Component 2', \n",
    "                        color='black', alpha=0.5, ax=ax)\n",
    "\n",
    "        ax.set_title(f'PCA - {model_name}')\n",
    "        ax.set_xlabel('Principal Component 1')\n",
    "        ax.set_ylabel('Principal Component 2')\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add a main title\n",
    "    fig.suptitle(f'PCA of Features - Original Dataset vs Synthetic Data ({synthetic_name.upper()})', \n",
    "                 fontsize=16, y=1.02)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'./images/pca_all_models_orig_vs_synth_{synthetic_name}.jpg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "create_scatter_pca(df_synth_raw, df_original,'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_remove_outliers and df_original are already loaded\n",
    "\n",
    "# Selecting numerical features for KL Divergence\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "# Ensure df_original has all columns from df_remove_outliers\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "# Add 'source' column\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "# Drop NaN values only from the synthetic data\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "# Combine the cleaned synthetic DataFrame with the original DataFrame\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df[features])\n",
    "\n",
    "# Create a DataFrame with the scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
    "\n",
    "# Add the source and model columns back to the scaled DataFrame\n",
    "scaled_df['source'] = combined_df['source'].values\n",
    "scaled_df['model'] = combined_df['model'].values\n",
    "\n",
    "# Function to calculate KL Divergence with smoothing\n",
    "def calculate_kl_divergence(p, q, epsilon=1e-10):\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "# Calculate KL Divergence for each feature within each model\n",
    "kl_results = {}\n",
    "models = df_synth_raw['model'].unique()\n",
    "\n",
    "for model_name in models:\n",
    "    kl_results[model_name] = {}\n",
    "    for feature in features:\n",
    "        # Extract data for the current model\n",
    "        synthetic_data = scaled_df[(scaled_df['model'] == model_name) & (scaled_df['source'] == 'synthetic')][feature]\n",
    "        original_data = scaled_df[scaled_df['source'] == 'original'][feature]\n",
    "        \n",
    "        # Calculate histograms (distributions) with smoothing\n",
    "        bins = 30\n",
    "        hist_synthetic, _ = np.histogram(synthetic_data, bins=bins, density=True)\n",
    "        hist_original, _ = np.histogram(original_data, bins=bins, density=True)\n",
    "\n",
    "        # Calculate KL Divergence\n",
    "        kl_divergence = calculate_kl_divergence(hist_synthetic, hist_original)\n",
    "        kl_results[model_name][feature] = kl_divergence\n",
    "\n",
    "# Convert the KL results into a DataFrame\n",
    "kl_df = pd.DataFrame(kl_results).T\n",
    "kl_df.reset_index(inplace=True)\n",
    "kl_df.rename(columns={'index': 'model'}, inplace=True)\n",
    "\n",
    "# Create a MultiIndex for the columns\n",
    "kl_df.columns = pd.MultiIndex.from_tuples([('Model', '')] + [('KL Divergence', feature) for feature in kl_df.columns[1:]])\n",
    "\n",
    "# Print KL Divergence DataFrame\n",
    "#kl_df\n",
    "\n",
    "# Calculate the mean of the KL divergence features for each row\n",
    "kl_df['median_kl'] = kl_df.iloc[:, 2:].median(axis=1)\n",
    "kl_df['mean_kl'] = kl_df.iloc[:, 2:].mean(axis=1)\n",
    "\n",
    "# Sort the DataFrame by this new column in descending order\n",
    "kl_df_sorted = kl_df.sort_values(by='median_kl', ascending=True)\n",
    "kl_df_sorted.dropna(inplace=True)\n",
    "\n",
    "# Drop the temporary mean column\n",
    "# kl_df_sorted = kl_df_sorted.drop(columns=['median_kl'])\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "kl_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stats.statistical_distances import (\n",
    "    calculate_distances,\n",
    "    nested_dict_to_df,\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df[features])\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
    "scaled_df['source'] = combined_df['source'].values\n",
    "scaled_df['model'] = combined_df['model'].values\n",
    "scaled_df['prompt_method'] = combined_df['prompt_method'].values\n",
    "scaled_df['sample_size'] = combined_df['sample_size'].values\n",
    "\n",
    "# Calculate distances for different groupings\n",
    "results_model = calculate_distances(scaled_df, ['model'], features)\n",
    "results_prompt = calculate_distances(scaled_df, ['prompt_method'], features)\n",
    "results_sample = calculate_distances(scaled_df, ['sample_size'], features)\n",
    "\n",
    "results_model_prompt = calculate_distances(scaled_df, ['model', 'prompt_method'], features)\n",
    "results_model_sample_size = calculate_distances(scaled_df, ['model', 'sample_size'], features)\n",
    "\n",
    "results_model_prompt_sample = calculate_distances(scaled_df, ['model', 'prompt_method', 'sample_size'], features)\n",
    "\n",
    "# Convert results to DataFrames\n",
    "df_model = nested_dict_to_df(results_model, ['model'])\n",
    "df_prompt = nested_dict_to_df(results_prompt, ['prompt_method'])\n",
    "df_sample = nested_dict_to_df(results_sample, ['sample_size'])\n",
    "\n",
    "df_model_prompt = nested_dict_to_df(results_model_prompt, ['model', 'prompt_method'])\n",
    "df_model_sample_size = nested_dict_to_df(results_model_sample_size, ['model', 'sample_size'])\n",
    "\n",
    "df_model_prompt_sample = nested_dict_to_df(results_model_prompt_sample, ['model', 'prompt_method', 'sample_size'])\n",
    "\n",
    "# Convert n_samples to integer\n",
    "for df in [df_model, df_model_prompt, df_model_prompt_sample]:\n",
    "    df['n_samples'] = df['n_samples'].astype(int)\n",
    "\n",
    "# Sort DataFrames by KL Divergence\n",
    "df_model_sorted = df_model.sort_values('KL Divergence')\n",
    "df_prompt_sorted = df_prompt.sort_values('KL Divergence')\n",
    "df_sample_size_sorted = df_sample.sort_values('KL Divergence')\n",
    "\n",
    "df_model_prompt_sorted = df_model_prompt.sort_values('KL Divergence')\n",
    "df_model_sample_size_sorted = df_model_sample_size.sort_values('KL Divergence')\n",
    "\n",
    "df_model_prompt_sample_sorted = df_model_prompt_sample.sort_values('KL Divergence')\n",
    "\n",
    "# Print results\n",
    "print(\"Results grouped by model:\")\n",
    "print(df_model_sorted)\n",
    "print(\"Results grouped by prompt:\")\n",
    "print(df_prompt_sorted)\n",
    "print(\"Results grouped by sample_size:\")\n",
    "print(df_sample_size_sorted)\n",
    "\n",
    "print(\"\\nResults grouped by model and prompt_method:\")\n",
    "print(df_model_prompt_sorted)\n",
    "print(\"\\nResults grouped by model and sample_size:\")\n",
    "print(df_model_sample_size_sorted)\n",
    "\n",
    "print(\"\\nResults grouped by model, prompt_method, and sample_size:\")\n",
    "print(df_model_prompt_sample_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sorted.to_csv('./distance_csv/distances_metrics_model_sorted_by_kl.csv')\n",
    "df_model_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prompt_sorted.to_csv('./distance_csv/distances_metrics_prompt_sorted_by_kl.csv')\n",
    "df_prompt_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_size_sorted.to_csv('./distance_csv/distances_metrics_sample_sorted_by_kl.csv')\n",
    "df_sample_size_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_prompt_sorted.to_csv('./distance_csv/distances_metrics_model_prompt_sorted_by_kl.csv')\n",
    "df_model_prompt_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sample_size_sorted.to_csv('./distance_csv/distances_metrics_model_sample_sorted_by_kl.csv')\n",
    "df_model_sample_size_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_prompt_sample_sorted.to_csv('./distance_csv/distances_metrics_model_prompt_sample_sorted_by_kl.csv')\n",
    "df_model_prompt_sample_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Assuming df_model_prompt_sorted is already loaded\n",
    "numeric_metrics = ['KL Divergence', 'Wasserstein', 'MMD']\n",
    "\n",
    "df_ordered = df_model_prompt_sorted\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig = plt.figure(figsize=(60, 15))\n",
    "\n",
    "for i, metric in enumerate(numeric_metrics, 1):\n",
    "    # Order models based on average metric value (reversed)\n",
    "    model_order = df_ordered.groupby('model')[metric].mean().sort_values(ascending=True).index\n",
    "\n",
    "    # Order prompt_method based on average metric value (reversed)\n",
    "    prompt_order = df_ordered.groupby('prompt_method')[metric].mean().sort_values(ascending=True).index\n",
    "\n",
    "    # Create numeric mappings for ordered models and prompt methods\n",
    "    model_map = {model: i for i, model in enumerate(reversed(model_order))}\n",
    "    prompt_map = {method: i for i, method in enumerate(prompt_order)}\n",
    "\n",
    "    # Create a grid of x, y coordinates\n",
    "    x = np.array([model_map[m] for m in df_ordered['model']])\n",
    "    y = np.array([prompt_map[p] for p in df_ordered['prompt_method']])\n",
    "    z = df_ordered[metric].values\n",
    "\n",
    "    # Create a grid for the surface plot\n",
    "    xi = np.linspace(x.min(), x.max(), 100)\n",
    "    yi = np.linspace(y.min(), y.max(), 100)\n",
    "    X, Y = np.meshgrid(xi, yi)\n",
    "\n",
    "    # Interpolate the z values on the grid\n",
    "    Z = griddata((x, y), z, (X, Y), method='cubic')\n",
    "\n",
    "    # Create 3D axes\n",
    "    ax = fig.add_subplot(1, 3, i, projection='3d')\n",
    "\n",
    "    # Create a custom colormap (inverted viridis)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 256))[::-1]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"inverted_viridis\", colors)\n",
    "\n",
    "    # Create the surface plot\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=custom_cmap, edgecolor='none', alpha=0.8)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_zlabel(metric, labelpad=20)\n",
    "    ax.set_title(f'3D Surface Plot: Model vs Prompt Method vs {metric}', pad=20)\n",
    "\n",
    "    # Set tick labels\n",
    "    ax.set_xticks(list(model_map.values()))\n",
    "    ax.set_xticklabels(list(model_map.keys()), rotation=45, ha='right', va='top')\n",
    "    ax.set_yticks(list(prompt_map.values()))\n",
    "    ax.set_yticklabels(list(prompt_map.keys()), rotation=-20, ha='left')\n",
    "\n",
    "    # Adjust tick label positions\n",
    "    ax.tick_params(axis='x', which='major', pad=8)\n",
    "    ax.tick_params(axis='y', which='major', pad=8)\n",
    "\n",
    "    # Add a color bar\n",
    "    cbar = fig.colorbar(surf, ax=ax, label=metric, pad=0.1, aspect=30)\n",
    "\n",
    "    # Set view angle\n",
    "    ax.view_init(elev=20, azim=-45)  # Adjusted view angle\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/3d_model_prompt_method.jpg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "numeric_metrics = ['KL Divergence', 'Wasserstein', 'MMD']\n",
    "\n",
    "index = 0\n",
    "df_3d = [df_model_sample_size_sorted.dropna(),df_model_prompt_sorted.dropna()][index]\n",
    "label = ['Sample Size','Prompt Method'][index]\n",
    "label_groupby = ['sample_size','prompt_method'][index]\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig = plt.figure(figsize=(15, 30))\n",
    "\n",
    "for i, metric in enumerate(numeric_metrics, 1):\n",
    "    # Order models based on average metric value (descending order)\n",
    "    model_order = df_3d.groupby('model')[metric].mean().sort_values(ascending=False).index\n",
    "\n",
    "    # Order labels based on average metric value (descending order)\n",
    "    label_order = df_3d.groupby(label_groupby)[metric].mean().sort_values(ascending=True).index\n",
    "\n",
    "    # Create numeric mappings for ordered models and labels\n",
    "    model_map = {model: i for i, model in enumerate(model_order)}\n",
    "    label_map = {size: i for i, size in enumerate(label_order)}\n",
    "\n",
    "    # Create a grid of x, y coordinates\n",
    "    x = np.array([model_map[m] for m in df_3d['model']])\n",
    "    y = np.array([label_map[s] for s in df_3d[label_groupby]])\n",
    "    z = df_3d[metric].values\n",
    "\n",
    "    # Create a grid for the surface plot\n",
    "    xi = np.linspace(x.min(), x.max(), 100)\n",
    "    yi = np.linspace(y.min(), y.max(), 100)\n",
    "    X, Y = np.meshgrid(xi, yi)\n",
    "\n",
    "    # Interpolate the z values on the grid\n",
    "    Z = griddata((x, y), z, (X, Y), method='cubic')\n",
    "\n",
    "    # Create 3D axes\n",
    "    ax = fig.add_subplot(3, 1, i, projection='3d')\n",
    "\n",
    "    # Create a custom colormap (inverted viridis)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 256))[::-1]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"inverted_viridis\", colors)\n",
    "\n",
    "    # Create the surface plot\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=custom_cmap, edgecolor='none', alpha=0.8)\n",
    "\n",
    "    # Set labels and title\n",
    "    # ax.set_xlabel('Model', labelpad=20)\n",
    "    # ax.set_ylabel(label, labelpad=20)\n",
    "    ax.set_zlabel(metric, labelpad=20)\n",
    "    ax.set_title(f'3D Surface Plot: Model vs {label} vs {metric}', pad=20)\n",
    "\n",
    "    # Set tick labels\n",
    "    ax.set_xticks(list(model_map.values()))\n",
    "    ax.set_xticklabels(list(model_map.keys()), rotation=45, ha='right', va='top')\n",
    "    ax.set_yticks(list(label_map.values()))\n",
    "    ax.set_yticklabels(list(label_map.keys()), rotation=-20, ha='left')\n",
    "\n",
    "    # Adjust tick label positions\n",
    "    ax.tick_params(axis='x', which='major', pad=8)\n",
    "    ax.tick_params(axis='y', which='major', pad=8)\n",
    "\n",
    "    # Set z-axis limits to match the range of metric values\n",
    "    z_min, z_max = df_3d[metric].min(), df_3d[metric].max()\n",
    "    ax.set_zlim(z_min, z_max)\n",
    "\n",
    "    # Add a color bar\n",
    "    cbar = fig.colorbar(surf, ax=ax, label=metric, pad=0.1, aspect=30)\n",
    "\n",
    "    # Set view angle\n",
    "    ax.view_init(elev=20, azim=-45)\n",
    "\n",
    "    # Print debugging information\n",
    "    print(f\"\\nMetric: {metric}\")\n",
    "    print(f\"{label} order:\", label_order)\n",
    "    print(\"Z-axis range:\", z_min, z_max)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./images/3dplot_multivar_{label_groupby}')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df[features])\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
    "scaled_df['source'] = combined_df['source'].values\n",
    "scaled_df['model'] = combined_df['model'].values\n",
    "scaled_df['prompt_method'] = combined_df['prompt_method'].values\n",
    "scaled_df['sample_size'] = combined_df['sample_size'].values\n",
    "\n",
    "\n",
    "\n",
    "from stats.statistical_distances import (\n",
    "    kl_divergence_permutation_test_knn,\n",
    "    multivariate_ks_test,\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "# model = 'claude-3-5-sonnet-20240620'\n",
    "# prompt_method = 'self_consistency'\n",
    "# sample_size = 'rs_size_150'\n",
    "\n",
    "def run_tests(X, Y, alpha_level=0.05):\n",
    "    # Perform the permutation test for KL divergence using KNN\n",
    "    kl_stat, kl_pvalue, kl_effect = kl_divergence_permutation_test_knn(Y, X, k=5, permutations=99)\n",
    "\n",
    "    # Perform the multivariate KS test\n",
    "    ks_stat, ks_pvalue, ks_effect = multivariate_ks_test(X, Y, permutations=99)\n",
    "\n",
    "    # Return the results in the desired format\n",
    "    results = {\n",
    "        'kl_statistic': kl_stat,\n",
    "        'ks_statistic': ks_stat,\n",
    "        'p_value_kl': kl_pvalue,\n",
    "        'p_value_ks': ks_pvalue,\n",
    "        'effect_size_kl': kl_effect,\n",
    "        'effect_size_ks': ks_effect,\n",
    "        'significant_5_kl': kl_pvalue < alpha_level,\n",
    "        'significant_5_ks': ks_pvalue < alpha_level,\n",
    "    }\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Get unique values for model, prompt_method, and sample_size\n",
    "models = scaled_df['model'].unique()\n",
    "prompt_methods = scaled_df['prompt_method'].unique()\n",
    "sample_sizes = scaled_df['sample_size'].unique()\n",
    "\n",
    "# Define a function to run the tests and store results\n",
    "def run_and_store_results(selector, model=None, prompt_method=None, sample_size=None):\n",
    "    X = scaled_df[(scaled_df['source'] == 'synthetic') & (selector)][features].values\n",
    "    Y = scaled_df[scaled_df['source'] == 'original'][features].values\n",
    "\n",
    "    if len(X) < 100:\n",
    "        print('Skipping because synthetic data has less than 100 samples!')\n",
    "        return\n",
    "\n",
    "    # Calculate statistics\n",
    "    print(f'Current Testing for Mode:l {model}, Prompt Method: {prompt_method}, Sample Size: {sample_size} ....')\n",
    "    results = run_tests(X, Y, 0.05)\n",
    "\n",
    "    # Add model, prompt_method, sample_size, and n_samples to results\n",
    "    results['model_name'] = model\n",
    "    results['prompt_method'] = prompt_method\n",
    "    results['sample_size'] = sample_size\n",
    "    results['n_samples'] = len(X)\n",
    "\n",
    "    results_list.append(results)\n",
    "\n",
    "# Progress bar setup\n",
    "total_combinations = len(models) * len(prompt_methods) * len(sample_sizes) + \\\n",
    "                     len(models) * len(prompt_methods) + \\\n",
    "                     len(models) * len(sample_sizes) + \\\n",
    "                     len(models) + \\\n",
    "                     len(prompt_methods) + \\\n",
    "                     len(sample_sizes)\n",
    "pbar = tqdm(total=total_combinations, desc=\"Processing combinations\")\n",
    "\n",
    "# Test for model, prompt_method, sample_size\n",
    "for model, prompt_method, sample_size in product(models, prompt_methods, sample_sizes):\n",
    "    selector = (scaled_df['model'] == model) & (scaled_df['prompt_method'] == prompt_method) & (scaled_df['sample_size'] == sample_size)\n",
    "    run_and_store_results(selector, model, prompt_method, sample_size)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Test for model, prompt_method\n",
    "for model, prompt_method in product(models, prompt_methods):\n",
    "    selector = (scaled_df['model'] == model) & (scaled_df['prompt_method'] == prompt_method)\n",
    "    run_and_store_results(selector, model, prompt_method, None)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Test for model, sample_size\n",
    "for model, sample_size in product(models, sample_sizes):\n",
    "    selector = (scaled_df['model'] == model) & (scaled_df['sample_size'] == sample_size)\n",
    "    run_and_store_results(selector, model, None, sample_size)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Test for model\n",
    "for model in models:\n",
    "    selector = (scaled_df['model'] == model)\n",
    "    run_and_store_results(selector, model, None, None)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Test for prompt_method\n",
    "for prompt_method in prompt_methods:\n",
    "    selector = (scaled_df['prompt_method'] == prompt_method)\n",
    "    run_and_store_results(selector, None, prompt_method, None)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Test for sample_size\n",
    "for sample_size in sample_sizes:\n",
    "    selector = (scaled_df['sample_size'] == sample_size)\n",
    "    run_and_store_results(selector, None, None, sample_size)\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Create DataFrame from results_list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Reorder columns as specified\n",
    "column_order = ['model_name', 'prompt_method', 'sample_size', 'n_samples', \n",
    "                'kl_statistic', 'ks_statistic', 'p_value_kl', 'p_value_ks', \n",
    "                'effect_size_kl', 'effect_size_ks', 'significant_5_kl', 'significant_5_ks']\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "results_df.to_csv('./testing_csv/hypothesis_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
