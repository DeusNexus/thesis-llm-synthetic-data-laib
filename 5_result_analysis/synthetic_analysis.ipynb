{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame length: 163300\n",
      "DataFrame length after removing outliers 'RAW' for Inf values: 159860 - Rows lost: 3440\n",
      "\n",
      "df_original head:   target_material  target_thickness  pulse_width  energy  spot_size  \\\n",
      "0         plastic             0.537         30.0   2.427        3.3   \n",
      "1         plastic             0.293         30.0   2.395        3.3   \n",
      "2         plastic             0.610         30.0   2.425        3.3   \n",
      "3         plastic             0.509         30.0   2.344        3.3   \n",
      "4         plastic             0.527         30.0   2.351        3.3   \n",
      "\n",
      "      intensity         power  cutoff_energy  \n",
      "0  6.561000e+20  8.091000e+13            3.3  \n",
      "1  6.473000e+20  7.983000e+13            3.4  \n",
      "2  6.554000e+20  8.083000e+13            3.4  \n",
      "3  6.335000e+20  7.813000e+13            3.4  \n",
      "4  6.356000e+20  7.838000e+13            3.4  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1067 entries, 0 to 1066\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   target_material   1067 non-null   object \n",
      " 1   target_thickness  1067 non-null   float64\n",
      " 2   pulse_width       1067 non-null   float64\n",
      " 3   energy            1067 non-null   float64\n",
      " 4   spot_size         1067 non-null   float64\n",
      " 5   intensity         1067 non-null   float64\n",
      " 6   power             1067 non-null   float64\n",
      " 7   cutoff_energy     1067 non-null   float64\n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 66.8+ KB\n",
      "\n",
      "df_original info: None\n",
      "\n",
      "df_original unique prompt_method: NA\n",
      "\n",
      "df_original unique prompt_short: NA\n",
      "\n",
      "df_original unique sample_size: NA\n",
      "\n",
      "df_original unique target_material: ['plastic' 'polystyrene' 'gold' 'aluminium' 'polypropylene']\n",
      "\n",
      "df_original unique model: NA\n",
      "\n",
      "df_synth_raw head:                         model     prompt_method prompt_short sample_size  \\\n",
      "0  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
      "1  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
      "2  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
      "3  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
      "4  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
      "\n",
      "  target_material  target_thickness  pulse_width  energy  spot_size  \\\n",
      "0         plastic             0.301         35.0    1.87        3.3   \n",
      "1            gold             0.685        180.0    3.25        3.3   \n",
      "2       aluminium             0.952         42.0    2.39        3.3   \n",
      "3   polypropylene             1.230        320.0   15.60        4.1   \n",
      "4         plastic             0.488         30.0    2.33        3.3   \n",
      "\n",
      "      intensity         power  cutoff_energy  \n",
      "0  5.120000e+20  5.340000e+13            4.2  \n",
      "1  7.210000e+20  1.810000e+13            6.8  \n",
      "2  6.380000e+20  5.690000e+13            5.1  \n",
      "3  1.450000e+21  4.880000e+13           18.3  \n",
      "4  6.300000e+20  7.770000e+13            4.5  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 159860 entries, 0 to 163297\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   model             159860 non-null  object \n",
      " 1   prompt_method     159860 non-null  object \n",
      " 2   prompt_short      159860 non-null  object \n",
      " 3   sample_size       159860 non-null  object \n",
      " 4   target_material   159860 non-null  object \n",
      " 5   target_thickness  159860 non-null  float64\n",
      " 6   pulse_width       159860 non-null  float64\n",
      " 7   energy            159860 non-null  float64\n",
      " 8   spot_size         159860 non-null  float64\n",
      " 9   intensity         159860 non-null  float64\n",
      " 10  power             159860 non-null  float64\n",
      " 11  cutoff_energy     159860 non-null  float64\n",
      "dtypes: float64(7), object(5)\n",
      "memory usage: 15.9+ MB\n",
      "\n",
      "df_synth_raw info: None\n",
      "\n",
      "df_synth_raw unique prompt_method: ['chain_of_thought' 'skeleton_of_thought' 'self_consistency'\n",
      " 'generated_knowledge' 'least_to_most' 'chain_of_verification'\n",
      " 'step_back_prompting' 'rephrase_and_respond' 'emotion_prompt'\n",
      " 'directional_stimuli' 'recursive_criticism_and_improvement'\n",
      " 'reverse_prompting']\n",
      "\n",
      "df_synth_raw unique prompt_short: ['cot' 'sot' 'sc' 'gk' 'ltm' 'cov' 'sbp' 'rar' 'em' 'ds' 'rcai' 'rp']\n",
      "\n",
      "df_synth_raw unique sample_size: ['rs_size_5' 'rs_size_10' 'rs_size_25' 'rs_size_50' 'rs_size_100'\n",
      " 'rs_size_150']\n",
      "\n",
      "df_synth_raw unique target_material: ['plastic' 'gold' 'aluminium' 'polypropylene' 'titanium' 'carbon' 'copper'\n",
      " 'silver' 'tantalum' 'polystyrene' 'aluminum' 'mylar' 'tungsten' 'nickel'\n",
      " 'silicon' 'iron' 'lead' 'beryllium' 'platinum' 'palladium' 'polyethylene'\n",
      " 'molybdenum' 'zinc' 'vanadium' 'indium' 'chromium' 'germanium' 'cobalt'\n",
      " 'tin' 'diamond' 'glass' 'lithium' 'deuterium' 'nanotube' 'boron' 'brass'\n",
      " 'kapton' 'magnesium' 'oxide' 'steel' 'graphene' 'polycarbonate'\n",
      " 'deuteratedplastic' 'alloy' 'graphite' 'zirconium' 'uranium' 'alumina'\n",
      " 'stainlesssteel' 'teflon' 'nylon' 'acrylic' 'ceramic' 'aramid' 'iridium'\n",
      " 'quartz' 'aluminumalloy' 'sapphire' 'nitride' 'zirconia' 'mica'\n",
      " 'gadolinium' 'parylene' 'rhodium' 'titaniumalloy' 'boroncarbide' 'inox'\n",
      " 'kevlar' 'ldpe' 'fiber' 'siliconnitride' 'boronnitride' 'gallium'\n",
      " 'tellurium' 'bismuth' 'peek' 'delrin' 'nomex' 'lexan' 'polyimide'\n",
      " 'carbide' 'osmium' 'rhenium' 'silica' 'hafnium' 'niobium' 'aluminumoxide'\n",
      " 'carbonfiber' 'rubber' 'siliconcarbide' 'titaniumnitride' 'sodium'\n",
      " 'potassium' 'pmma' 'manganese' 'magnesiumalloy' 'macor' 'fusedsilica'\n",
      " 'borosilicate' 'yttrium' 'galliumarsenide' 'viton' 'inconel' 'ruthenium'\n",
      " 'cadmium' 'scandium' 'tungstencarbide' 'zircaloy' 'bronze' 'chrome'\n",
      " 'aerogel' 'ptfe' 'ch' 'pvc' 'borosilicateglass' 'pudieran' 'materials'\n",
      " 'for' 'methods' 'before' 'on' 'example' 'order' 'material' 'paper'\n",
      " 'array' 'neon' 'fluorine' 'the' 'process' 'as' 'analysis' 'possibility'\n",
      " 'in' 'features' 'syntheticmaterial' 'metal' 'pet' 'pfa' 'pvdf' 'abs'\n",
      " 'psu' 'pps' 'pai' 'pei' 'lcp' 'hdpe' 'pe' 'neoprene' 'titanate'\n",
      " 'limestone' 'bronz' 'plutonium' 'mercury' 'polyester' 'ruby' 'emerald'\n",
      " 'polyvinylchloride' 'wood' 'epoxy' 'thorium' 'pp' 'pc' 'plexiglass'\n",
      " 'argon' 'hydrogen' 'nitrogen' 'oxygen' 'helium' 'neodymium'\n",
      " 'vulcanizedrubber' 'silicone' 'butyl' 'cotton' 'nitinol' 'polonium'\n",
      " 'cerium' 'fiberglass' 'acetal' 'polymethylmethacrylate' 'barium'\n",
      " 'thallium' 'selenium' 'vandium' 'polymer' 'cement' 'cesium'\n",
      " 'polyurethane' 'bamboo' 'bronzite' 'polyamide' 'agate' 'opal' 'amethyst'\n",
      " 'topaz' 'pearl' 'brimstone' 'silk' 'wool' 'linen' 'marble' 'wolfram'\n",
      " 'aluminosilicate' 'rubidium' 'strontium' 'resin' 'polycarbon' 'latex'\n",
      " 'acetate' 'plastics' 'plastyrene' 'elastic' 'paraxylene' 'plastik'\n",
      " 'skavkov' 'plastolina' 'plasteron' 'pyroelectric' 'foil' 'polythene' 'ic'\n",
      " 'e' 'polyscrene' 'qualdo' 'plastc' 'plastin' 'ceramics' 'styrofoam'\n",
      " 'cdse' 'steal' 'simcon' 'ytrrium' 'polyerien' 'aluminumfoil'\n",
      " 'profiledtarget' 'copolymer' 'eicosane' 'goldd' 'transparentglass'\n",
      " 'phosphor' 'gelatin' 'plast' 'sandwich' 'plastinlet' 'plastlight'\n",
      " 'somemat' 'csi' 'peel' 'peuli' 'polystone' 'krypton' 'pulsat'\n",
      " 'polypropane' 'polypropene' 'buckette' 'auliminium' 'polypylene'\n",
      " 'alkuminium' 'keratin' 'materialx' 'phynyl' 'guns' 'gun' 'visible'\n",
      " 'hydrogenated' 'metalizedch' 'waterjet' 'nickelalloy' 'perovskite'\n",
      " 'crystallinesi' 'lications' 'owerfer' 'xenon' 'stone' 'clay'\n",
      " 'aluminiumoxide' 'titaniumdioxide' 'standardousolspectulsaws'\n",
      " 'targetmaterial' 'aware' 'relatedtargetsize' 'uropa' 'exposing' 'ware'\n",
      " 'generated' 'energy' 'rior' 'following' 'pulsing' 'riorductolarator'\n",
      " 'ularatedulse' 'ulastic' 'ulferriorws' 'ulaterictherrors' 'uls'\n",
      " 'generate' 'fer' 'by' 'thecut' 'issample' 'targetsamples' 'this'\n",
      " 'arsenic' 'bromine' 'phosphorus' 'sulfur' 'ironoxide' 'nickeloxide'\n",
      " 'copperoxide' 'terbium' 'dysprosium' 'erbium' 'cuttargetsamples'\n",
      " 'followingsamplesize' 'cuts' 'targetident' 'targets' 'cells' 'tissue'\n",
      " 'and' 'thickness' 'specifically' 'target' 'spot' 'repartition' 'other'\n",
      " 'antimony' 'dioxide' 'seleinium' 'iodine' 'plasic' 'composite' 'water'\n",
      " 'arsenide' 'sulfide' 'pentoxide' 'silicondioxide' 'titaniumoxide' 'air'\n",
      " 'metalalloy' 'polymercompound' 'polymercomposite' 'polymerblend'\n",
      " 'polymersolution' 'polymergel' 'ceramiccomposite' 'targetthickness'\n",
      " 'pulsewidth' 'spotsize' 'sulfate' 'goldoxide' 'siliconoxide' 'titananium'\n",
      " 'silicongermanium' 'zincsulfide' 'silicontitanate' 'aluminumnitride'\n",
      " 'galliumarsenideoxide' 'fluoride' 'terephthalate' 'aluminide' 'chloride'\n",
      " 'styrene' 'naphthalate' 'polytetrafluoroethylene' 'polyphenylsulfone'\n",
      " 'polysulfone' 'goldgallium' 'siliconboron' 'indiumtinoxide'\n",
      " 'silversulfide' 'goldarsenic' 'silverselenide' 'indiumoxide'\n",
      " 'galliumarsenidegalliumantimonide' 'silicongermaniumcarbon' 'aluiminium'\n",
      " 'alu' 'aluinium' 'polystyren' 'aluuminium' 'polystyrenene' 'al' 'pl' 'au'\n",
      " 'polyestyrens' 'aluoxide' 'aluuminum' 'disulfide' 'minium' 'polystyrenne'\n",
      " 'polystyrenen' 'aluinum' 'aluni' 'alualloy' 'nanotubes' 'alufoil'\n",
      " 'polyethyleneterephthalate' 'polyamideimide' 'polyetheretherketone'\n",
      " 'polyoxymethylene' 'polyacrylonitrile' 'polyvinylfluoride'\n",
      " 'polyvinylidenechloride' 'polyvinylalcohol' 'polyethylenenaphthalate'\n",
      " 'polystyrenesulfonic' 'polyethyleneoxide' 'polypropylenesulfide'\n",
      " 'polyethyleneglycol' 'polyethylenetetrafluoroethylene' 'polyacrylicacid'\n",
      " 'polyestyrene' 'molybdenumtrioxide' 'tungstentrioxide' 'carbondioxide'\n",
      " 'ammonia' 'ethanol' 'isopropanol' 'polyprop' 'alunitride'\n",
      " 'polyethersulfone' 'polyetherimide' 'imide' 'polyarylamide'\n",
      " 'polyarylsulfone' 'polyphthalamide' 'methacrylate' 'phenolic' 'urethane'\n",
      " 'polystyrenes' 'polystyrense' 'polysty' 'cu' 'fe' 'pt' 'ag' 'ps'\n",
      " 'polystyrenefoam' 'petg' 'polybenzimidazole' 'nitrate' 'lutetium'\n",
      " 'thulium' 'iodide' 'nanoparticles' 'mrk']\n",
      "\n",
      "df_synth_raw unique model: ['claude-3-5-sonnet-20240620' 'claude-3-sonnet-20240229' 'gemma:7b'\n",
      " 'gpt-3.5-turbo-0125' 'gpt-4o' 'llama2:13b' 'llama3:8b' 'llama3:70b'\n",
      " 'mistral:7b' 'mixtral:8x22b' 'phi3:medium-128k' 'phi3:mini-128k']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_method</th>\n",
       "      <th>prompt_short</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>target_material</th>\n",
       "      <th>target_thickness</th>\n",
       "      <th>pulse_width</th>\n",
       "      <th>energy</th>\n",
       "      <th>spot_size</th>\n",
       "      <th>intensity</th>\n",
       "      <th>power</th>\n",
       "      <th>cutoff_energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>chain_of_thought</td>\n",
       "      <td>cot</td>\n",
       "      <td>rs_size_5</td>\n",
       "      <td>plastic</td>\n",
       "      <td>0.301</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.120000e+20</td>\n",
       "      <td>5.340000e+13</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>chain_of_thought</td>\n",
       "      <td>cot</td>\n",
       "      <td>rs_size_5</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.685</td>\n",
       "      <td>180.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.3</td>\n",
       "      <td>7.210000e+20</td>\n",
       "      <td>1.810000e+13</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>chain_of_thought</td>\n",
       "      <td>cot</td>\n",
       "      <td>rs_size_5</td>\n",
       "      <td>aluminium</td>\n",
       "      <td>0.952</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.380000e+20</td>\n",
       "      <td>5.690000e+13</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>chain_of_thought</td>\n",
       "      <td>cot</td>\n",
       "      <td>rs_size_5</td>\n",
       "      <td>polypropylene</td>\n",
       "      <td>1.230</td>\n",
       "      <td>320.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.450000e+21</td>\n",
       "      <td>4.880000e+13</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>chain_of_thought</td>\n",
       "      <td>cot</td>\n",
       "      <td>rs_size_5</td>\n",
       "      <td>plastic</td>\n",
       "      <td>0.488</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.300000e+20</td>\n",
       "      <td>7.770000e+13</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model     prompt_method prompt_short sample_size  \\\n",
       "0  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
       "1  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
       "2  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
       "3  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
       "4  claude-3-5-sonnet-20240620  chain_of_thought          cot   rs_size_5   \n",
       "\n",
       "  target_material  target_thickness  pulse_width  energy  spot_size  \\\n",
       "0         plastic             0.301         35.0    1.87        3.3   \n",
       "1            gold             0.685        180.0    3.25        3.3   \n",
       "2       aluminium             0.952         42.0    2.39        3.3   \n",
       "3   polypropylene             1.230        320.0   15.60        4.1   \n",
       "4         plastic             0.488         30.0    2.33        3.3   \n",
       "\n",
       "      intensity         power  cutoff_energy  \n",
       "0  5.120000e+20  5.340000e+13            4.2  \n",
       "1  7.210000e+20  1.810000e+13            6.8  \n",
       "2  6.380000e+20  5.690000e+13            5.1  \n",
       "3  1.450000e+21  4.880000e+13           18.3  \n",
       "4  6.300000e+20  7.770000e+13            4.5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Use ggplot\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "features = ['target_material','target_thickness','pulse_width','energy','spot_size','intensity','power','cutoff_energy']\n",
    "numeric_features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "# Load original dataset that was used to generate samples\n",
    "df_original = pd.read_csv('../1_sample_preparation/source/d_clean_remove_small_samples.csv')\n",
    "# Make sure required features are numeric\n",
    "df_original[numeric_features] = df_original[numeric_features].astype(float)\n",
    "df_original.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Load synthetic data set\n",
    "df_synthetic = pd.read_csv('../4_response_extraction/synthetic_data_rows.csv')\n",
    "# Drop column\n",
    "df_synthetic.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "# Make sure required features are numeric\n",
    "df_synthetic[numeric_features] = df_synthetic[numeric_features].astype(float)\n",
    "df_synthetic.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_synth_raw = df_synthetic.copy()\n",
    "\n",
    "# Function to remove outliers for a specific feature\n",
    "def remove_outliers(df, feature, lower_percentile, upper_percentile):\n",
    "    Q1 = df[feature].quantile(lower_percentile)\n",
    "    Q3 = df[feature].quantile(upper_percentile)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "\n",
    "# Remove outliers for all numeric features\n",
    "for nf in numeric_features:\n",
    "    df_synth_raw = remove_outliers(df_synth_raw, nf, 0.005, 0.995)\n",
    "\n",
    "print(f\"Original DataFrame length: {len(df_synthetic)}\")\n",
    "print(f\"DataFrame length after removing outliers 'RAW' for Inf values: {len(df_synth_raw)} - Rows lost: {len(df_synthetic) - len(df_synth_raw)}\")\n",
    "\n",
    "print(\"\\ndf_original head:\",df_original.head())\n",
    "print(\"\\ndf_original info:\",df_original.info())\n",
    "print(\"\\ndf_original unique prompt_method: NA\")\n",
    "print(\"\\ndf_original unique prompt_short: NA\")\n",
    "print(\"\\ndf_original unique sample_size: NA\")\n",
    "print(\"\\ndf_original unique target_material:\",df_original['target_material'].unique())\n",
    "print(\"\\ndf_original unique model: NA\")\n",
    "\n",
    "print(\"\\ndf_synth_raw head:\",df_synth_raw.head())\n",
    "print(\"\\ndf_synth_raw info:\",df_synth_raw.info())\n",
    "print(\"\\ndf_synth_raw unique prompt_method:\",df_synth_raw['prompt_method'].unique())\n",
    "print(\"\\ndf_synth_raw unique prompt_short:\",df_synth_raw['prompt_short'].unique())\n",
    "print(\"\\ndf_synth_raw unique sample_size:\",df_synth_raw['sample_size'].unique())\n",
    "print(\"\\ndf_synth_raw unique target_material:\",df_synth_raw['target_material'].unique())\n",
    "print(\"\\ndf_synth_raw unique model:\",df_synth_raw['model'].unique())\n",
    "\n",
    "df_synthetic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each 'target_material'\n",
    "material_counts = df_synth_raw['target_material'].value_counts()\n",
    "\n",
    "print('Unique Materials:',len(df_synth_raw['target_material'].unique()))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width = 800, height = 400, background_color ='white')\n",
    "\n",
    "# Generate a word cloud using the frequencies\n",
    "wordcloud.generate_from_frequencies(material_counts.to_dict())\n",
    "\n",
    "# Display the WordCloud image:\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.title('Wordcloud of frequent target_material (all models)',y=1.05)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./images/word_cloud_target_material.jpg',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each 'target_material'\n",
    "material_counts = df_synth_raw['target_material'].value_counts()\n",
    "\n",
    "# Assuming 'material_counts' holds the value counts of 'target_material' from your DataFrame\n",
    "top_25_materials = material_counts.head(50)  # Get the top 50 materials for better illustration\n",
    "\n",
    "# Create a DataFrame from the top 25 materials for easier plotting\n",
    "top_25_df = pd.DataFrame(top_25_materials).reset_index()\n",
    "top_25_df.columns = ['target_material', 'count']\n",
    "\n",
    "# Calculate cumulative percentage\n",
    "total_rows = df_synthetic.shape[0]  # Total number of rows in the original DataFrame\n",
    "top_25_df['cumulative_percent'] = top_25_df['count'].cumsum() / total_rows * 100\n",
    "\n",
    "# Initialize the matplotlib figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)  # Share y-axis across subplots\n",
    "\n",
    "# Left subplot for frequency\n",
    "sns.barplot(x='count', y='target_material', data=top_25_df, ax=axes[0], hue='target_material',palette='viridis')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Frequency (Log Scale)')\n",
    "axes[0].set_ylabel('Material')\n",
    "axes[0].set_title('Top 50 Frequent Materials')\n",
    "\n",
    "# Right subplot for cumulative percentage\n",
    "sns.lineplot(x='cumulative_percent', y='target_material', data=top_25_df, ax=axes[1],color='black')\n",
    "axes[1].set_xlabel('Cumulative Percentage (%)')\n",
    "axes[1].set_title('Total Share of Rows by Material')\n",
    "\n",
    "# Set x-axis to have a tick every 5 percent\n",
    "axes[1].xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "\n",
    "# Optionally, you can format the ticks to show them as percentages\n",
    "axes[1].xaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# If you want to limit the x-axis to show up to 100% only\n",
    "axes[1].set_xlim(40, 100)\n",
    "\n",
    "# Annotate each point on the line plot\n",
    "for index, row in top_25_df.iterrows():\n",
    "    axes[1].annotate(f\"{row['cumulative_percent']:.1f}%\", \n",
    "                     (row['cumulative_percent'], row['target_material']), \n",
    "                     textcoords=\"offset points\", \n",
    "                     xytext=(-14,-10), \n",
    "                     ha='center', \n",
    "                     color='black', \n",
    "                     size=7,\n",
    "    )\n",
    "# Plotting the points as black dots\n",
    "axes[1].scatter(top_25_df['cumulative_percent'], top_25_df['target_material'], color='black', s=20)  # s is the size of the dot\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('./images/top_50_materials_comparison.jpg', dpi=300)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "num_models = len(df_synth_raw['model'].unique())\n",
    "num_features = len(numeric_features)\n",
    "\n",
    "# Calculate the optimal figure size\n",
    "fig_width = min(20, 2.5 * num_features)  # Cap the width at 20 inches\n",
    "fig_height = min(30, 2 * num_models)  # Cap the height at 30 inches\n",
    "\n",
    "# Initialize the matplotlib figure with subplots\n",
    "fig, axes = plt.subplots(num_models, num_features, figsize=(fig_width, fig_height))\n",
    "\n",
    "# If there's only one model, wrap axes in a list to make it 2D\n",
    "if num_models == 1:\n",
    "    axes = np.array([axes])\n",
    "\n",
    "for i, model_name in enumerate(df_synth_raw['model'].unique()):\n",
    "    # Filter the DataFrame for the specific model\n",
    "    model_df = df_synth_raw[df_synth_raw['model'] == model_name].copy()\n",
    "\n",
    "    for j, feature in enumerate(numeric_features):\n",
    "        # Create a boxplot for the numeric features of this specific model\n",
    "        sns.boxplot(data=model_df, y=feature, ax=axes[i, j], orient='v', color='red', fliersize=2)\n",
    "        \n",
    "        # Set the xlabel on each subplot\n",
    "        axes[i, j].set_xlabel('')\n",
    "        \n",
    "        # Only set the feature name for the bottom row\n",
    "        if i == num_models - 1:\n",
    "            axes[i, j].set_xlabel(feature, fontsize=10, fontweight='normal')\n",
    "        \n",
    "        # Remove y-label for all subplots\n",
    "        axes[i, j].set_ylabel('')\n",
    "\n",
    "        # Rotate x-axis labels for better readability\n",
    "        axes[i, j].tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        axes[i, j].tick_params(axis='y', labelsize=8)\n",
    "\n",
    "        if i % 2 != 0:\n",
    "            axes[i, j].patch.set_facecolor((0.95, 0.95, 0.95))  # Light grey color\n",
    "        else:\n",
    "            axes[i, j].patch.set_facecolor((0.99, 0.99, 0.99))  # Light grey color\n",
    "\n",
    "    # Add model name to the top left corner of each row\n",
    "    axes[i, 0].text(-0.3, 1.125, model_name, transform=axes[i, 0].transAxes, \n",
    "                    fontsize=12, fontweight='light', ha='left', va='bottom')\n",
    "\n",
    "    # Extend the shaded background to include the model name area for shaded rows\n",
    "    if i % 2 == 0:\n",
    "        axes[i, 0].add_patch(plt.Rectangle((-0.35, -0.1), 10, 1.4, \n",
    "                                           fill=True, transform=axes[i, 0].transAxes, \n",
    "                                           facecolor=(0.96, 0.96, 0.96), clip_on=False, zorder=0))\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.435, top=0.935, left=0.15)\n",
    "\n",
    "# Add a main title\n",
    "fig.suptitle('Feature Distribution Across Models - Raw', fontsize=16, fontweight='bold', y=0.975)\n",
    "fig.text(s='Raw data which might include extreme outliers due to mistakes large language models can introduce.',y=0.957,x=0.255,fontsize=12, fontweight='light')\n",
    "\n",
    "# Save the figure with high resolution\n",
    "plt.savefig('./images/boxplot_outliers_models_raw.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_kde_plot(data, title, subtitle):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    synthetic_models = [model for model in data['model'].unique() if model != 'original data']\n",
    "    color_palette = sns.color_palette(\"husl\", n_colors=len(synthetic_models))\n",
    "\n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Determine the x-axis range\n",
    "        x_min = max(0, data[feature].quantile(0.0025))  # Use 1st percentile, but never go below 0\n",
    "        x_max = data[feature].quantile(0.9975)  # Use 99th percentile to exclude extreme outliers\n",
    "        x_range = x_max - x_min\n",
    "        x_padding = x_range * 0.1  # Add 10% padding on each side\n",
    "        \n",
    "        original_data = data[data['model'] == 'original data']\n",
    "        if not original_data.empty:\n",
    "            sns.kdeplot(data=original_data, x=feature, ax=ax, color='black', alpha=1, label='Original Data', bw_adjust=1.5)\n",
    "        \n",
    "        for j, model in enumerate(synthetic_models):\n",
    "            model_data = data[data['model'] == model]\n",
    "\n",
    "            if model_data[feature].nunique() == 1:\n",
    "                print(f\"  Skipping plot for {model} - all values are the same\")\n",
    "                continue\n",
    "            \n",
    "            sns.kdeplot(data=model_data, x=feature, ax=ax, color=color_palette[j], alpha=0.7, label=model, bw_adjust=1.5)\n",
    "        \n",
    "        ax.set_title(f\"log({feature})\", fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Density', fontsize=12)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.set_xlim(x_min - x_padding, x_max + x_padding)\n",
    "        ax.legend().remove()\n",
    "        \n",
    "        # Remove scientific notation from x-axis\n",
    "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[-1].legend(handles, labels, title='Models', title_fontsize='15', fontsize='13', loc='center', bbox_to_anchor=(0.5, 0.5), bbox_transform=axes[-1].transAxes)\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(title, fontsize=20, fontweight='bold', y=1.02)\n",
    "    fig.text(0.5, 0.97, subtitle, ha='center', fontsize=14, fontstyle='italic')\n",
    "    plt.subplots_adjust(top=0.9, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    return fig\n",
    "\n",
    "df_original_log = df_original.copy()\n",
    "df_synth_raw_log = df_synth_raw.copy()\n",
    "\n",
    "# Add 'model' column to df_original and select only numeric features\n",
    "df_original_log['model'] = 'original data'\n",
    "df_original_log = df_original_log[['model'] + numeric_features]\n",
    "\n",
    "# Log transform numeric features\n",
    "for feature in numeric_features:\n",
    "    df_original_log[feature]            = np.log1p(df_original_log[feature])\n",
    "    df_synth_raw_log[feature] = np.log1p(df_synth_raw[feature])\n",
    "\n",
    "# Add original data to cleaned dataframes\n",
    "df_clean_raw_with_original = pd.concat([df_synth_raw_log, df_original_log], ignore_index=True)\n",
    "\n",
    "# Create and save plots for raw data (without inf/NaN values), IQR-cleaned data, and IPR-cleaned data\n",
    "raw_plot = create_kde_plot(df_clean_raw_with_original, 'Kernel Density Estimation of Log-Transformed Features Across Models (Raw Data)', 'Extreme outliers removed (0.5th to 99.5th percentile)')\n",
    "raw_plot.savefig('./images/kde_features_across_models_raw_log.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_remove_outliers and df_original are already loaded\n",
    "\n",
    "# Selecting numerical features for PCA\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "# Ensure df_original has all columns from df_remove_outliers\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "# Add 'source' column\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "# Drop NaN values only from the synthetic data\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "# Combine the cleaned synthetic DataFrame with the original DataFrame\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "# Now proceed with PCA on the combined DataFrame\n",
    "X = combined_df[features].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "# Standardizing the features\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Proceed with PCA\n",
    "pca = PCA(n_components=2)  # Example: reducing to 2 principal components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# Adding the source and model information back to the PCA DataFrame\n",
    "pca_df = pd.concat([pca_df, combined_df.reset_index(drop=True)[['source', 'model']]], axis=1)\n",
    "\n",
    "# Handle NaNs in the 'model' column for original data\n",
    "pca_df['model'].fillna('original', inplace=True)\n",
    "\n",
    "# Separate the synthetic and original data points\n",
    "synthetic_points = pca_df[pca_df['source'] == 'synthetic']\n",
    "original_points = pca_df[pca_df['source'] == 'original']\n",
    "\n",
    "# Plotting the PCA result\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.xlim([-2,20])\n",
    "\n",
    "# Plot synthetic points first\n",
    "sns.scatterplot(data=synthetic_points, x='Principal Component 1', y='Principal Component 2', hue='model', style='model', palette='Set1', alpha=0.5, legend=True)\n",
    "\n",
    "# Plot original points on top in black\n",
    "sns.scatterplot(data=original_points, x='Principal Component 1', y='Principal Component 2', color='black')\n",
    "\n",
    "plt.title('PCA of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def create_scatter_pca(synthetic, original,synthetic_name):\n",
    "    # Selecting numerical features for PCA\n",
    "    features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "    # Ensure original has all columns from synthetic\n",
    "    missing_cols = set(synthetic.columns) - set(original.columns)\n",
    "    for col in missing_cols:\n",
    "        original[col] = np.nan\n",
    "\n",
    "    # Add 'source' column\n",
    "    synthetic['source'] = 'synthetic'\n",
    "    original['source'] = 'original'\n",
    "\n",
    "    # Drop NaN values only from the synthetic data\n",
    "    df_synth_raw_copy = synthetic.copy()\n",
    "    df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "    # Combine the cleaned synthetic DataFrame with the original DataFrame\n",
    "    combined_df = pd.concat([df_synth_raw_copy, original], ignore_index=True)\n",
    "\n",
    "    # Now proceed with PCA on the combined DataFrame\n",
    "    X = combined_df[features].copy()\n",
    "\n",
    "    # Standardizing the features\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Proceed with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Create a DataFrame with the principal components\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "    # Adding the source and model information back to the PCA DataFrame\n",
    "    pca_df = pd.concat([pca_df, combined_df.reset_index(drop=True)[['source', 'model']]], axis=1)\n",
    "\n",
    "    # Handle NaNs in the 'model' column for original data\n",
    "    pca_df['model'].fillna('original', inplace=True)\n",
    "\n",
    "    # Separate the synthetic and original data points\n",
    "    synthetic_points = pca_df[pca_df['source'] == 'synthetic']\n",
    "    original_points = pca_df[pca_df['source'] == 'original']\n",
    "\n",
    "    # Get unique models\n",
    "    unique_models = combined_df['model'].unique()\n",
    "    unique_models = [model for model in unique_models if isinstance(model, str)]\n",
    "\n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_models = len(unique_models)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes to iterate easily\n",
    "\n",
    "    for i, model_name in enumerate(unique_models):\n",
    "        ax = axes[i]\n",
    "        # Plot synthetic points first\n",
    "        sns.scatterplot(data=synthetic_points[synthetic_points['model'] == model_name], \n",
    "                        x='Principal Component 1', y='Principal Component 2', \n",
    "                        hue='model', style='model', palette='Set1', alpha=0.5, \n",
    "                        legend=False, ax=ax)\n",
    "\n",
    "        # Plot original points on top in black\n",
    "        sns.scatterplot(data=original_points, x='Principal Component 1', y='Principal Component 2', \n",
    "                        color='black', alpha=0.5, ax=ax)\n",
    "\n",
    "        ax.set_title(f'PCA - {model_name}')\n",
    "        ax.set_xlabel('Principal Component 1')\n",
    "        ax.set_ylabel('Principal Component 2')\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add a main title\n",
    "    fig.suptitle(f'PCA of Features - Original Dataset vs Synthetic Data ({synthetic_name.upper()})', \n",
    "                 fontsize=16, y=1.02)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'./images/pca_all_models_orig_vs_synth_{synthetic_name}.jpg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "create_scatter_pca(df_synth_raw, df_original,'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_remove_outliers and df_original are already loaded\n",
    "\n",
    "# Selecting numerical features for KL Divergence\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "\n",
    "# Ensure df_original has all columns from df_remove_outliers\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "# Add 'source' column\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "# Drop NaN values only from the synthetic data\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "# Combine the cleaned synthetic DataFrame with the original DataFrame\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df[features])\n",
    "\n",
    "# Create a DataFrame with the scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
    "\n",
    "# Add the source and model columns back to the scaled DataFrame\n",
    "scaled_df['source'] = combined_df['source'].values\n",
    "scaled_df['model'] = combined_df['model'].values\n",
    "\n",
    "# Function to calculate KL Divergence with smoothing\n",
    "def calculate_kl_divergence(p, q, epsilon=1e-10):\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "# Calculate KL Divergence for each feature within each model\n",
    "kl_results = {}\n",
    "models = df_synth_raw['model'].unique()\n",
    "\n",
    "for model_name in models:\n",
    "    kl_results[model_name] = {}\n",
    "    for feature in features:\n",
    "        # Extract data for the current model\n",
    "        synthetic_data = scaled_df[(scaled_df['model'] == model_name) & (scaled_df['source'] == 'synthetic')][feature]\n",
    "        original_data = scaled_df[scaled_df['source'] == 'original'][feature]\n",
    "        \n",
    "        # Calculate histograms (distributions) with smoothing\n",
    "        bins = 30\n",
    "        hist_synthetic, _ = np.histogram(synthetic_data, bins=bins, density=True)\n",
    "        hist_original, _ = np.histogram(original_data, bins=bins, density=True)\n",
    "\n",
    "        # Calculate KL Divergence\n",
    "        kl_divergence = calculate_kl_divergence(hist_synthetic, hist_original)\n",
    "        kl_results[model_name][feature] = kl_divergence\n",
    "\n",
    "# Convert the KL results into a DataFrame\n",
    "kl_df = pd.DataFrame(kl_results).T\n",
    "kl_df.reset_index(inplace=True)\n",
    "kl_df.rename(columns={'index': 'model'}, inplace=True)\n",
    "\n",
    "# Create a MultiIndex for the columns\n",
    "kl_df.columns = pd.MultiIndex.from_tuples([('Model', '')] + [('KL Divergence', feature) for feature in kl_df.columns[1:]])\n",
    "\n",
    "# Print KL Divergence DataFrame\n",
    "#kl_df\n",
    "\n",
    "# Calculate the mean of the KL divergence features for each row\n",
    "kl_df['median_kl'] = kl_df.iloc[:, 2:].median(axis=1)\n",
    "kl_df['mean_kl'] = kl_df.iloc[:, 2:].mean(axis=1)\n",
    "\n",
    "# Sort the DataFrame by this new column in descending order\n",
    "kl_df_sorted = kl_df.sort_values(by='median_kl', ascending=True)\n",
    "kl_df_sorted.dropna(inplace=True)\n",
    "\n",
    "# Drop the temporary mean column\n",
    "# kl_df_sorted = kl_df_sorted.drop(columns=['median_kl'])\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "kl_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import rel_entr\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def calculate_wasserstein_distance(x, y):\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    # Calculate Wasserstein distance for each dimension\n",
    "    distances = [wasserstein_distance(x[:, i], y[:, i]) for i in range(x.shape[1])]\n",
    "    \n",
    "    # Return the average distance across all dimensions\n",
    "    return np.mean(distances)\n",
    "\n",
    "# Function to calculate KL Divergence with smoothing\n",
    "def calculate_kl_divergence(p, q, epsilon=1e-10):\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "# Function to calculate Maximum Mean Discrepancy (MMD) (memory-efficient version)\n",
    "def calculate_mmd(x, y, kernel='rbf', batch_size=1000):\n",
    "    nx, ny = len(x), len(y)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    gamma = 1.0 / x.shape[1]\n",
    "    \n",
    "    total_kxx = 0\n",
    "    total_kyy = 0\n",
    "    total_kxy = 0\n",
    "    \n",
    "    for i in range(0, nx, batch_size):\n",
    "        xi = x[i:i+batch_size]\n",
    "        kxx = np.mean(np.exp(-gamma * euclidean_distances(xi, xi, squared=True)))\n",
    "        kxy = np.mean(np.exp(-gamma * euclidean_distances(xi, y, squared=True)))\n",
    "        total_kxx += kxx * len(xi)\n",
    "        total_kxy += kxy * len(xi)\n",
    "    \n",
    "    for i in range(0, ny, batch_size):\n",
    "        yi = y[i:i+batch_size]\n",
    "        kyy = np.mean(np.exp(-gamma * euclidean_distances(yi, yi, squared=True)))\n",
    "        total_kyy += kyy * len(yi)\n",
    "    \n",
    "    total_kxx /= nx\n",
    "    total_kyy /= ny\n",
    "    total_kxy /= (nx * ny)\n",
    "    \n",
    "    return total_kxx + total_kyy - 2 * total_kxy\n",
    "\n",
    "# Updated multivariate distance calculation\n",
    "def calculate_multivariate_distance(synthetic_data, original_data, method):\n",
    "    if len(synthetic_data) == 0 or len(original_data) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    synthetic_data = np.asarray(synthetic_data)\n",
    "    original_data = np.asarray(original_data)\n",
    "    \n",
    "    if method == 'kl':\n",
    "        hist_synthetic, _ = np.histogramdd(synthetic_data, bins=[10]*synthetic_data.shape[1], density=True)\n",
    "        hist_original, _ = np.histogramdd(original_data, bins=[10]*original_data.shape[1], density=True)\n",
    "        return calculate_kl_divergence(hist_synthetic.flatten(), hist_original.flatten())\n",
    "    elif method == 'wasserstein':\n",
    "        return calculate_wasserstein_distance(synthetic_data, original_data)\n",
    "    elif method == 'mmd':\n",
    "        return calculate_mmd(synthetic_data, original_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "# Updated main calculation loop\n",
    "def calculate_distances(scaled_df, grouping_columns, features):\n",
    "    results = {}\n",
    "    \n",
    "    for group, group_data in scaled_df[scaled_df['source'] == 'synthetic'].groupby(grouping_columns):\n",
    "        synthetic_data = group_data\n",
    "        original_data = scaled_df[scaled_df['source'] == 'original']\n",
    "        \n",
    "        # Count samples\n",
    "        n_samples = len(synthetic_data)\n",
    "        \n",
    "        # Multivariate calculations\n",
    "        synthetic_multivariate = synthetic_data[features].values\n",
    "        original_multivariate = original_data[features].values\n",
    "\n",
    "        print(f\"Calculating for group: {group}\")\n",
    "        print(f\"Synthetic data shape: {synthetic_multivariate.shape}\")\n",
    "        print(f\"Original data shape: {original_multivariate.shape}\")\n",
    "        \n",
    "        group_results = {\n",
    "            'n_samples': n_samples,\n",
    "            'multivariate_kl': calculate_multivariate_distance(synthetic_multivariate, original_multivariate, 'kl'),\n",
    "            'multivariate_wasserstein': calculate_multivariate_distance(synthetic_multivariate, original_multivariate, 'wasserstein'),\n",
    "            'multivariate_mmd': calculate_multivariate_distance(synthetic_multivariate, original_multivariate, 'mmd')\n",
    "        }\n",
    "\n",
    "        print(f\"Results for group {group}: {group_results}\")\n",
    "        \n",
    "        # Handle different grouping levels\n",
    "        if isinstance(group, tuple):\n",
    "            current_level = results\n",
    "            for level in group[:-1]:\n",
    "                if level not in current_level:\n",
    "                    current_level[level] = {}\n",
    "                current_level = current_level[level]\n",
    "            current_level[group[-1]] = group_results\n",
    "        else:\n",
    "            results[group] = group_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Updated function to convert nested dictionary to DataFrame\n",
    "def nested_dict_to_df(d, index_names):\n",
    "    rows = []\n",
    "    def process_dict(current_dict, current_row):\n",
    "        if isinstance(current_dict, dict):\n",
    "            if all(isinstance(v, (int, float)) for v in current_dict.values()):\n",
    "                # This is a leaf node with metric values\n",
    "                rows.append(current_row + list(current_dict.values()))\n",
    "            else:\n",
    "                for key, value in current_dict.items():\n",
    "                    new_row = current_row + [key]\n",
    "                    process_dict(value, new_row)\n",
    "        else:\n",
    "            # This handles the case where a value might be a single number\n",
    "            rows.append(current_row + [current_dict])\n",
    "    \n",
    "    process_dict(d, [])\n",
    "    df = pd.DataFrame(rows, columns=index_names + ['n_samples', 'KL Divergence', 'Wasserstein', 'MMD'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df[features])\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
    "scaled_df['source'] = combined_df['source'].values\n",
    "scaled_df['model'] = combined_df['model'].values\n",
    "scaled_df['prompt_method'] = combined_df['prompt_method'].values\n",
    "scaled_df['sample_size'] = combined_df['sample_size'].values\n",
    "\n",
    "# Calculate distances for different groupings\n",
    "results_model = calculate_distances(scaled_df, ['model'], features)\n",
    "results_prompt = calculate_distances(scaled_df, ['prompt_method'], features)\n",
    "results_sample = calculate_distances(scaled_df, ['sample_size'], features)\n",
    "\n",
    "results_model_prompt = calculate_distances(scaled_df, ['model', 'prompt_method'], features)\n",
    "results_model_sample_size = calculate_distances(scaled_df, ['model', 'sample_size'], features)\n",
    "\n",
    "results_model_prompt_sample = calculate_distances(scaled_df, ['model', 'prompt_method', 'sample_size'], features)\n",
    "\n",
    "# Convert results to DataFrames\n",
    "df_model = nested_dict_to_df(results_model, ['model'])\n",
    "df_prompt = nested_dict_to_df(results_prompt, ['prompt_method'])\n",
    "df_sample = nested_dict_to_df(results_sample, ['sample_size'])\n",
    "\n",
    "df_model_prompt = nested_dict_to_df(results_model_prompt, ['model', 'prompt_method'])\n",
    "df_model_sample_size = nested_dict_to_df(results_model_sample_size, ['model', 'sample_size'])\n",
    "\n",
    "df_model_prompt_sample = nested_dict_to_df(results_model_prompt_sample, ['model', 'prompt_method', 'sample_size'])\n",
    "\n",
    "# Convert n_samples to integer\n",
    "for df in [df_model, df_model_prompt, df_model_prompt_sample]:\n",
    "    df['n_samples'] = df['n_samples'].astype(int)\n",
    "\n",
    "# Sort DataFrames by KL Divergence\n",
    "df_model_sorted = df_model.sort_values('KL Divergence')\n",
    "df_prompt_sorted = df_prompt.sort_values('KL Divergence')\n",
    "df_sample_size_sorted = df_sample.sort_values('KL Divergence')\n",
    "\n",
    "df_model_prompt_sorted = df_model_prompt.sort_values('KL Divergence')\n",
    "df_model_sample_size_sorted = df_model_sample_size.sort_values('KL Divergence')\n",
    "\n",
    "df_model_prompt_sample_sorted = df_model_prompt_sample.sort_values('KL Divergence')\n",
    "\n",
    "# Print results\n",
    "print(\"Results grouped by model:\")\n",
    "print(df_model_sorted)\n",
    "print(\"Results grouped by prompt:\")\n",
    "print(df_prompt_sorted)\n",
    "print(\"Results grouped by sample_size:\")\n",
    "print(df_sample_size_sorted)\n",
    "\n",
    "print(\"\\nResults grouped by model and prompt_method:\")\n",
    "print(df_model_prompt_sorted)\n",
    "print(\"\\nResults grouped by model and sample_size:\")\n",
    "print(df_model_sample_size_sorted)\n",
    "\n",
    "print(\"\\nResults grouped by model, prompt_method, and sample_size:\")\n",
    "print(df_model_prompt_sample_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prompt_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_size_sorted.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_prompt_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sample_size_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_prompt_sample_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Assuming df_model_prompt_sorted is already loaded\n",
    "numeric_metrics = ['KL Divergence', 'Wasserstein', 'MMD']\n",
    "\n",
    "df_ordered = df_model_prompt_sorted\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig = plt.figure(figsize=(60, 15))\n",
    "\n",
    "for i, metric in enumerate(numeric_metrics, 1):\n",
    "    # Order models based on average metric value (reversed)\n",
    "    model_order = df_ordered.groupby('model')[metric].mean().sort_values(ascending=True).index\n",
    "\n",
    "    # Order prompt_method based on average metric value (reversed)\n",
    "    prompt_order = df_ordered.groupby('prompt_method')[metric].mean().sort_values(ascending=True).index\n",
    "\n",
    "    # Create numeric mappings for ordered models and prompt methods\n",
    "    model_map = {model: i for i, model in enumerate(reversed(model_order))}\n",
    "    prompt_map = {method: i for i, method in enumerate(prompt_order)}\n",
    "\n",
    "    # Create a grid of x, y coordinates\n",
    "    x = np.array([model_map[m] for m in df_ordered['model']])\n",
    "    y = np.array([prompt_map[p] for p in df_ordered['prompt_method']])\n",
    "    z = df_ordered[metric].values\n",
    "\n",
    "    # Create a grid for the surface plot\n",
    "    xi = np.linspace(x.min(), x.max(), 100)\n",
    "    yi = np.linspace(y.min(), y.max(), 100)\n",
    "    X, Y = np.meshgrid(xi, yi)\n",
    "\n",
    "    # Interpolate the z values on the grid\n",
    "    Z = griddata((x, y), z, (X, Y), method='cubic')\n",
    "\n",
    "    # Create 3D axes\n",
    "    ax = fig.add_subplot(1, 3, i, projection='3d')\n",
    "\n",
    "    # Create a custom colormap (inverted viridis)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 256))[::-1]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"inverted_viridis\", colors)\n",
    "\n",
    "    # Create the surface plot\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=custom_cmap, edgecolor='none', alpha=0.8)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_zlabel(metric, labelpad=20)\n",
    "    ax.set_title(f'3D Surface Plot: Model vs Prompt Method vs {metric}', pad=20)\n",
    "\n",
    "    # Set tick labels\n",
    "    ax.set_xticks(list(model_map.values()))\n",
    "    ax.set_xticklabels(list(model_map.keys()), rotation=45, ha='right', va='top')\n",
    "    ax.set_yticks(list(prompt_map.values()))\n",
    "    ax.set_yticklabels(list(prompt_map.keys()), rotation=-20, ha='left')\n",
    "\n",
    "    # Adjust tick label positions\n",
    "    ax.tick_params(axis='x', which='major', pad=8)\n",
    "    ax.tick_params(axis='y', which='major', pad=8)\n",
    "\n",
    "    # Add a color bar\n",
    "    cbar = fig.colorbar(surf, ax=ax, label=metric, pad=0.1, aspect=30)\n",
    "\n",
    "    # Set view angle\n",
    "    ax.view_init(elev=20, azim=-45)  # Adjusted view angle\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_model_sample_size_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m numeric_metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKL Divergence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWasserstein\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMMD\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m df_3d \u001b[38;5;241m=\u001b[39m [\u001b[43mdf_model_sample_size_sorted\u001b[49m\u001b[38;5;241m.\u001b[39mdropna(),df_model_prompt_sorted\u001b[38;5;241m.\u001b[39mdropna()][index]\n\u001b[1;32m     12\u001b[0m label \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample Size\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrompt Method\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\n\u001b[1;32m     13\u001b[0m label_groupby \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_method\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_model_sample_size_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "numeric_metrics = ['KL Divergence', 'Wasserstein', 'MMD']\n",
    "\n",
    "index = 0\n",
    "df_3d = [df_model_sample_size_sorted.dropna(),df_model_prompt_sorted.dropna()][index]\n",
    "label = ['Sample Size','Prompt Method'][index]\n",
    "label_groupby = ['sample_size','prompt_method'][index]\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig = plt.figure(figsize=(15, 30))\n",
    "\n",
    "for i, metric in enumerate(numeric_metrics, 1):\n",
    "    # Order models based on average metric value (descending order)\n",
    "    model_order = df_3d.groupby('model')[metric].mean().sort_values(ascending=False).index\n",
    "\n",
    "    # Order labels based on average metric value (descending order)\n",
    "    label_order = df_3d.groupby(label_groupby)[metric].mean().sort_values(ascending=True).index\n",
    "\n",
    "    # Create numeric mappings for ordered models and labels\n",
    "    model_map = {model: i for i, model in enumerate(model_order)}\n",
    "    label_map = {size: i for i, size in enumerate(label_order)}\n",
    "\n",
    "    # Create a grid of x, y coordinates\n",
    "    x = np.array([model_map[m] for m in df_3d['model']])\n",
    "    y = np.array([label_map[s] for s in df_3d[label_groupby]])\n",
    "    z = df_3d[metric].values\n",
    "\n",
    "    # Create a grid for the surface plot\n",
    "    xi = np.linspace(x.min(), x.max(), 100)\n",
    "    yi = np.linspace(y.min(), y.max(), 100)\n",
    "    X, Y = np.meshgrid(xi, yi)\n",
    "\n",
    "    # Interpolate the z values on the grid\n",
    "    Z = griddata((x, y), z, (X, Y), method='cubic')\n",
    "\n",
    "    # Create 3D axes\n",
    "    ax = fig.add_subplot(3, 1, i, projection='3d')\n",
    "\n",
    "    # Create a custom colormap (inverted viridis)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, 256))[::-1]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list(\"inverted_viridis\", colors)\n",
    "\n",
    "    # Create the surface plot\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=custom_cmap, edgecolor='none', alpha=0.8)\n",
    "\n",
    "    # Set labels and title\n",
    "    # ax.set_xlabel('Model', labelpad=20)\n",
    "    # ax.set_ylabel(label, labelpad=20)\n",
    "    ax.set_zlabel(metric, labelpad=20)\n",
    "    ax.set_title(f'3D Surface Plot: Model vs {label} vs {metric}', pad=20)\n",
    "\n",
    "    # Set tick labels\n",
    "    ax.set_xticks(list(model_map.values()))\n",
    "    ax.set_xticklabels(list(model_map.keys()), rotation=45, ha='right', va='top')\n",
    "    ax.set_yticks(list(label_map.values()))\n",
    "    ax.set_yticklabels(list(label_map.keys()), rotation=-20, ha='left')\n",
    "\n",
    "    # Adjust tick label positions\n",
    "    ax.tick_params(axis='x', which='major', pad=8)\n",
    "    ax.tick_params(axis='y', which='major', pad=8)\n",
    "\n",
    "    # Set z-axis limits to match the range of metric values\n",
    "    z_min, z_max = df_3d[metric].min(), df_3d[metric].max()\n",
    "    ax.set_zlim(z_min, z_max)\n",
    "\n",
    "    # Add a color bar\n",
    "    cbar = fig.colorbar(surf, ax=ax, label=metric, pad=0.1, aspect=30)\n",
    "\n",
    "    # Set view angle\n",
    "    ax.view_init(elev=20, azim=-45)\n",
    "\n",
    "    # Print debugging information\n",
    "    print(f\"\\nMetric: {metric}\")\n",
    "    print(f\"{label} order:\", label_order)\n",
    "    print(\"Z-axis range:\", z_min, z_max)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./images/3dplot_multivar_{label_groupby}')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def calculate_ci_and_effect_size(statistic_func, x, y, alpha=0.05, n_bootstrap=2):\n",
    "    print('Calculating Boostrap Distributions for CI and Effect Size...')\n",
    "    combined = np.vstack([x, y])\n",
    "    n = len(x)\n",
    "    observed = statistic_func(x, y)\n",
    "    \n",
    "    # Bootstrap for confidence interval\n",
    "    bootstrap_stats = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_x = combined[np.random.choice(len(combined), n, replace=True)]\n",
    "        boot_y = combined[np.random.choice(len(combined), len(y), replace=True)]\n",
    "        bootstrap_stats.append(statistic_func(boot_x, boot_y))\n",
    "    \n",
    "    ci_lower, ci_upper = np.percentile(bootstrap_stats, [alpha/2 * 100, (1 - alpha/2) * 100])\n",
    "    \n",
    "    # Effect size (modified Cohen's d)\n",
    "    bootstrap_std = np.std(bootstrap_stats)\n",
    "    if bootstrap_std == 0:\n",
    "        # If standard deviation is zero, use a small constant to avoid division by zero\n",
    "        effect_size = (observed - np.mean(bootstrap_stats)) / 1e-8\n",
    "    else:\n",
    "        effect_size = (observed - np.mean(bootstrap_stats)) / bootstrap_std\n",
    "    \n",
    "    return ci_lower, ci_upper, effect_size\n",
    "\n",
    "# Multivariate Kolmogorov-Smirnov Test\n",
    "def multivariate_ks_test(x, y, min_samples=5):\n",
    "    print('Calculating KS Test')\n",
    "\n",
    "    # Check if either dataset has fewer than min_samples\n",
    "    if len(x) < min_samples or len(y) < min_samples:\n",
    "        print(f\"Warning: Not enough samples for KL divergence test. x: {len(x)}, y: {len(y)}\")\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    n, d = x.shape\n",
    "    m, _ = y.shape\n",
    "    \n",
    "    def ks_statistic(x, y):\n",
    "        z = np.vstack([x, y])\n",
    "        idx = np.argsort(z, axis=0)\n",
    "        cdf_x = np.sum(idx < n, axis=1) / n\n",
    "        cdf_y = np.sum(idx >= n, axis=1) / m\n",
    "        return np.max(np.abs(cdf_x - cdf_y))\n",
    "    \n",
    "    observed = ks_statistic(x, y)\n",
    "    \n",
    "    # Approximate p-value using the asymptotic distribution\n",
    "    n_eff = n * m / (n + m)\n",
    "    p_value = np.exp(-2 * n_eff * observed**2)\n",
    "    \n",
    "    ci_lower, ci_upper, effect_size = calculate_ci_and_effect_size(ks_statistic, x, y)\n",
    "    \n",
    "    return observed, p_value, ci_lower, ci_upper, effect_size\n",
    "\n",
    "# Multivariate KL Divergence Test\n",
    "def multivariate_kl_divergence_test(x, y, num_permutations=100, k=5, epsilon=1e-10, min_samples=5):\n",
    "    print('Calculating KL Test')\n",
    "    \n",
    "    # Check if either dataset has fewer than min_samples\n",
    "    if len(x) < min_samples or len(y) < min_samples:\n",
    "        print(f\"Warning: Not enough samples for KL divergence test. x: {len(x)}, y: {len(y)}\")\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    def kl_divergence(x, y):\n",
    "        n, m = len(x), len(y)\n",
    "        d = x.shape[1]\n",
    "        \n",
    "        cx = cKDTree(x)\n",
    "        cy = cKDTree(y)\n",
    "        \n",
    "        dx, _ = cx.query(x, k=min(k+1, n), eps=0, p=2)\n",
    "        dy, _ = cy.query(y, k=min(k+1, m), eps=0, p=2)\n",
    "        \n",
    "        dx = dx[:, -1]\n",
    "        dy = dy[:, -1]\n",
    "        \n",
    "        nx = np.array([len(cx.query_ball_point(point, r=dx[i], p=2)) for i, point in enumerate(x)])\n",
    "        ny = np.array([len(cy.query_ball_point(point, r=dx[i], p=2)) for i, point in enumerate(x)])\n",
    "        \n",
    "        # Add small epsilon to avoid log(0)\n",
    "        nx = np.maximum(nx, epsilon)\n",
    "        ny = np.maximum(ny, epsilon)\n",
    "        \n",
    "        kl = np.mean(np.log(ny) - np.log(nx)) + d * np.log(m / (n - 1))\n",
    "        return np.maximum(kl, epsilon)  # Return at least epsilon\n",
    "    \n",
    "    observed = kl_divergence(x, y)\n",
    "    combined = np.vstack([x, y])\n",
    "    n = len(x)\n",
    "    \n",
    "    permutation_stats = np.array([\n",
    "        kl_divergence(combined[np.random.permutation(len(combined))[:n]], \n",
    "                      combined[np.random.permutation(len(combined))[n:]])\n",
    "        for _ in range(num_permutations)\n",
    "    ])\n",
    "    \n",
    "    p_value = np.mean(permutation_stats >= observed)\n",
    "    ci_lower, ci_upper, effect_size = calculate_ci_and_effect_size(kl_divergence, x, y)\n",
    "    \n",
    "    return observed, p_value, ci_lower, ci_upper, effect_size\n",
    "\n",
    "# Multivariate Anderson-Darling Test\n",
    "def multivariate_anderson_darling_test(x, y, num_permutations=20, min_samples=5):\n",
    "    print('Calculating Anderson-Darling Test')\n",
    "\n",
    "    # Check if either dataset has fewer than min_samples\n",
    "    if len(x) < min_samples or len(y) < min_samples:\n",
    "        print(f\"Warning: Not enough samples for KL divergence test. x: {len(x)}, y: {len(y)}\")\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    def ad_statistic(x, y):\n",
    "        n, m = len(x), len(y)\n",
    "        N = n + m\n",
    "        Z = np.vstack([x, y])\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        D = squareform(pdist(Z))\n",
    "        \n",
    "        # Compute the number of x and y points within each pairwise distance\n",
    "        ix = np.zeros(N)\n",
    "        iy = np.zeros(N)\n",
    "        \n",
    "        for i in range(N):\n",
    "            ix[i] = (D[i, :n] <= D[i, i]).sum() - (i < n)\n",
    "            iy[i] = (D[i, n:] <= D[i, i]).sum() - (i >= n)\n",
    "        \n",
    "        # Compute the AD statistic\n",
    "        ad = (1 / (n * m)) * np.sum((ix * (N - iy) - (n - ix) * iy)**2 / ((ix + iy) * (N - ix - iy) + 1e-8))\n",
    "        return ad\n",
    "\n",
    "    observed = ad_statistic(x, y)\n",
    "    combined = np.vstack([x, y])\n",
    "    n = len(x)\n",
    "    \n",
    "    permutation_stats = np.array([\n",
    "        ad_statistic(combined[np.random.permutation(len(combined))[:n]], \n",
    "                     combined[np.random.permutation(len(combined))[n:]])\n",
    "        for _ in range(num_permutations)\n",
    "    ])\n",
    "    \n",
    "    p_value = np.mean(permutation_stats >= observed)\n",
    "    ci_lower, ci_upper, effect_size = calculate_ci_and_effect_size(ad_statistic, x, y)\n",
    "    \n",
    "    return observed, p_value, ci_lower, ci_upper, effect_size\n",
    "\n",
    "# Updated calculate_distances function\n",
    "def calculate_distances(scaled_df, grouping_columns, features):\n",
    "    results = {}\n",
    "    \n",
    "    for group, group_data in scaled_df[scaled_df['source'] == 'synthetic'].groupby(grouping_columns):\n",
    "        synthetic_data = group_data\n",
    "        original_data = scaled_df[scaled_df['source'] == 'original']\n",
    "        \n",
    "        n_samples = len(synthetic_data)\n",
    "        \n",
    "        synthetic_multivariate = synthetic_data[features].values\n",
    "        original_multivariate = original_data[features].values\n",
    "\n",
    "        print(f\"Calculating for group: {group}\")\n",
    "        print(f\"Synthetic data shape: {synthetic_multivariate.shape}\")\n",
    "        print(f\"Original data shape: {original_multivariate.shape}\")\n",
    "        \n",
    "        # Perform tests\n",
    "        ks_stat, ks_p, ks_ci_lower, ks_ci_upper, ks_effect = multivariate_ks_test(synthetic_multivariate, original_multivariate)\n",
    "        kl_stat, kl_p, kl_ci_lower, kl_ci_upper, kl_effect = multivariate_kl_divergence_test(synthetic_multivariate, original_multivariate)\n",
    "        ad_stat, ad_p, ad_ci_lower, ad_ci_upper, ad_effect = multivariate_anderson_darling_test(synthetic_multivariate, original_multivariate)\n",
    "        \n",
    "        group_results = {\n",
    "            'n_samples': n_samples,\n",
    "            'ks_statistic': ks_stat,\n",
    "            'ks_p_value': ks_p,\n",
    "            'ks_ci_lower': ks_ci_lower,\n",
    "            'ks_ci_upper': ks_ci_upper,\n",
    "            'ks_effect_size': ks_effect,\n",
    "            'kl_statistic': kl_stat,\n",
    "            'kl_p_value': kl_p,\n",
    "            'kl_ci_lower': kl_ci_lower,\n",
    "            'kl_ci_upper': kl_ci_upper,\n",
    "            'kl_effect_size': kl_effect,\n",
    "            'ad_statistic': ad_stat,\n",
    "            'ad_p_value': ad_p,\n",
    "            'ad_ci_lower': ad_ci_lower,\n",
    "            'ad_ci_upper': ad_ci_upper,\n",
    "            'ad_effect_size': ad_effect\n",
    "        }\n",
    "\n",
    "        print(f\"Results for group {group}: {group_results}\")\n",
    "        \n",
    "        # Handle different grouping levels (as before)\n",
    "        if isinstance(group, tuple):\n",
    "            current_level = results\n",
    "            for level in group[:-1]:\n",
    "                if level not in current_level:\n",
    "                    current_level[level] = {}\n",
    "                current_level = current_level[level]\n",
    "            current_level[group[-1]] = group_results\n",
    "        else:\n",
    "            results[group] = group_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Updated nested_dict_to_df function\n",
    "def nested_dict_to_df(d, index_names):\n",
    "    rows = []\n",
    "    def process_dict(current_dict, current_row):\n",
    "        if isinstance(current_dict, dict):\n",
    "            if all(isinstance(v, (int, float)) for v in current_dict.values()):\n",
    "                rows.append(current_row + list(current_dict.values()))\n",
    "            else:\n",
    "                for key, value in current_dict.items():\n",
    "                    new_row = current_row + [key]\n",
    "                    process_dict(value, new_row)\n",
    "        else:\n",
    "            rows.append(current_row + [current_dict])\n",
    "    \n",
    "    process_dict(d, [])\n",
    "    df = pd.DataFrame(rows, columns=index_names + ['n_samples', 'KS Statistic', 'KS p-value',\n",
    "                                                   'KL Statistic', 'KL p-value',\n",
    "                                                   'AD Statistic', 'AD p-value'])\n",
    "    return df\n",
    "\n",
    "# Add significance indicators\n",
    "def add_significance(df, alpha=0.05):\n",
    "    df['KS Significant'] = df['KS p-value'] < alpha\n",
    "    df['KL Significant'] = df['KL p-value'] < alpha\n",
    "    df['AD Significant'] = df['AD p-value'] < alpha\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for group: ('claude-3-5-sonnet-20240620',)\n",
      "Synthetic data shape: (26997, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('claude-3-5-sonnet-20240620',): {'n_samples': 26997, 'ks_statistic': 0.002663456757439709, 'ks_p_value': 0.9855425011325379, 'ks_ci_lower': 0.002663456757439709, 'ks_ci_upper': 0.002663456757439709, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 9.486000782701375, 'ad_p_value': 1.0, 'ad_ci_lower': 33.383384191833116, 'ad_ci_upper': 35.9158116372059, 'ad_effect_size': -18.879448901019362}\n",
      "Calculating for group: ('claude-3-sonnet-20240229',)\n",
      "Synthetic data shape: (24200, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('claude-3-sonnet-20240229',): {'n_samples': 24200, 'ks_statistic': 0.0036248615489477723, 'ks_p_value': 0.9735015589524626, 'ks_ci_lower': 0.0036248615489477723, 'ks_ci_upper': 0.0036248615489477723, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 4.183519547844098, 'ad_p_value': 1.0, 'ad_ci_lower': 27.68834427919909, 'ad_ci_upper': 28.42568119585972, 'ad_effect_size': -61.51819614001416}\n",
      "Calculating for group: ('gemma:7b',)\n",
      "Synthetic data shape: (1118, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('gemma:7b',): {'n_samples': 1118, 'ks_statistic': 0.006560449859418931, 'ks_p_value': 0.9540921303997495, 'ks_ci_lower': 0.006560449859418931, 'ks_ci_upper': 0.006560449859418931, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 20.26687851761249, 'ad_p_value': 0.0, 'ad_ci_lower': 3.625261525556014, 'ad_ci_upper': 3.809528434797645, 'ad_effect_size': 170.64387116785494}\n",
      "Calculating for group: ('gpt-3.5-turbo-0125',)\n",
      "Synthetic data shape: (23080, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('gpt-3.5-turbo-0125',): {'n_samples': 23080, 'ks_statistic': 0.003618845822119063, 'ks_p_value': 0.9736415624487639, 'ks_ci_lower': 0.003618845822119063, 'ks_ci_upper': 0.003618845822119063, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 6.9805187558708575, 'ad_p_value': 1.0, 'ad_ci_lower': 29.745879056128146, 'ad_ci_upper': 29.945513912090597, 'ad_effect_size': -217.6164952468226}\n",
      "Calculating for group: ('gpt-4o',)\n",
      "Synthetic data shape: (23498, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('gpt-4o',): {'n_samples': 23498, 'ks_statistic': 0.0036211580510590825, 'ks_p_value': 0.9735878325834023, 'ks_ci_lower': 0.0036211580510590825, 'ks_ci_upper': 0.0036211580510590825, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 0.9026952974002864, 'ad_p_value': 1.0, 'ad_ci_lower': 23.743218231661018, 'ad_ci_upper': 24.282111300433797, 'ad_effect_size': -81.47987891257088}\n",
      "Calculating for group: ('llama2:13b',)\n",
      "Synthetic data shape: (3260, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('llama2:13b',): {'n_samples': 3260, 'ks_statistic': 0.00531649427038713, 'ks_p_value': 0.9555731862472348, 'ks_ci_lower': 0.005347593160112925, 'ks_ci_upper': 0.006529350969693136, 'ks_effect_size': -1.0000000000000007, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 67.21064867652997, 'ad_p_value': 0.0, 'ad_ci_lower': 28.318364860960138, 'ad_ci_upper': 28.332562957547164, 'ad_effect_size': 5203.644770618983}\n",
      "Calculating for group: ('llama3:70b',)\n",
      "Synthetic data shape: (11697, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('llama3:70b',): {'n_samples': 11697, 'ks_statistic': 0.00451505160087588, 'ks_p_value': 0.9609177628276035, 'ks_ci_lower': 0.00451505160087588, 'ks_ci_upper': 0.00451505160087588, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 4.7233711419419375, 'ad_p_value': 1.0, 'ad_ci_lower': 15.985814669351466, 'ad_ci_upper': 16.458151231164276, 'ad_effect_size': -46.253803330301}\n",
      "Calculating for group: ('llama3:8b',)\n",
      "Synthetic data shape: (13397, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('llama3:8b',): {'n_samples': 13397, 'ks_statistic': 0.0035248977603359144, 'ks_p_value': 0.9757403431339051, 'ks_ci_lower': 0.004536748460030253, 'ks_ci_upper': 0.004536748460030253, 'ks_effect_size': -101185.06996943384, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 3.278178619031397, 'ad_p_value': 1.0, 'ad_ci_lower': 16.058671840192982, 'ad_ci_upper': 17.095185968319427, 'ad_effect_size': -24.3775022995583}\n",
      "Calculating for group: ('mistral:7b',)\n",
      "Synthetic data shape: (3158, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('mistral:7b',): {'n_samples': 3158, 'ks_statistic': 0.006560449859418931, 'ks_p_value': 0.9336524382837246, 'ks_ci_lower': 0.005337933206037775, 'ks_ci_upper': 0.006529103278563004, 'ks_effect_size': 1.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 0.05734405144498557, 'ad_p_value': 1.0, 'ad_ci_lower': 3.4379950387241793, 'ad_ci_upper': 3.7537815350004617, 'ad_effect_size': -21.290441885807414}\n",
      "Calculating for group: ('mixtral:8x22b',)\n",
      "Synthetic data shape: (18918, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('mixtral:8x22b',): {'n_samples': 18918, 'ks_statistic': 0.00359024936011017, 'ks_p_value': 0.9742976515447223, 'ks_ci_lower': 0.00359024936011017, 'ks_ci_upper': 0.00359024936011017, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 0.44564741301567695, 'ad_p_value': 1.0, 'ad_ci_lower': 18.828644534501876, 'ad_ci_upper': 18.940087349063155, 'ad_effect_size': -314.36360740327456}\n",
      "Calculating for group: ('phi3:medium-128k',)\n",
      "Synthetic data shape: (10478, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('phi3:medium-128k',): {'n_samples': 10478, 'ks_statistic': 0.004495159492473452, 'ks_p_value': 0.9616205570613443, 'ks_ci_lower': 0.004495159492473452, 'ks_ci_upper': 0.004495159492473452, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 0.0018833978582468217, 'ad_p_value': 1.0, 'ad_ci_lower': 10.45608443750638, 'ad_ci_upper': 10.647192611850096, 'ad_effect_size': -104.88580517182343}\n",
      "Calculating for group: ('phi3:mini-128k',)\n",
      "Synthetic data shape: (59, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('phi3:mini-128k',): {'n_samples': 59, 'ks_statistic': 0.10075770813146316, 'ks_p_value': 0.3213643831921247, 'ks_ci_lower': 0.04754578812765079, 'ks_ci_upper': 0.06453782980954045, 'ks_effect_size': 5.000000000000002, 'kl_statistic': 18.960472905458634, 'kl_p_value': 1.0, 'kl_ci_lower': 22.98564397298692, 'kl_ci_upper': 23.08631505748606, 'kl_effect_size': -76.91843787223621, 'ad_statistic': 0.9171707226332759, 'ad_p_value': 1.0, 'ad_ci_lower': 0.7913349701314429, 'ad_ci_upper': 0.8829970295105956, 'ad_effect_size': 1.6583630607132134}\n",
      "Calculating for group: ('chain_of_thought',)\n",
      "Synthetic data shape: (10261, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('chain_of_thought',): {'n_samples': 10261, 'ks_statistic': 0.004491122837338163, 'ks_p_value': 0.961761406469188, 'ks_ci_lower': 0.0034823259140737894, 'ks_ci_upper': 0.004465256249562154, 'ks_effect_size': 1.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 0.9227050348497083, 'ad_p_value': 1.0, 'ad_ci_lower': 11.184079553002858, 'ad_ci_upper': 11.319567179909411, 'ad_effect_size': -144.84957245275112}\n",
      "Calculating for group: ('chain_of_verification',)\n",
      "Synthetic data shape: (10807, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('chain_of_verification',): {'n_samples': 10807, 'ks_statistic': 0.004500970378375153, 'ks_p_value': 0.9614167602683547, 'ks_ci_lower': 0.004500970378375153, 'ks_ci_upper': 0.004500970378375153, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 1.570525837349376, 'ad_p_value': 1.0, 'ad_ci_lower': 12.42727861097644, 'ad_ci_upper': 12.474558509806911, 'ad_effect_size': -437.24175992645206}\n",
      "Calculating for group: ('directional_stimuli',)\n",
      "Synthetic data shape: (14275, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('directional_stimuli',): {'n_samples': 14275, 'ks_statistic': 0.0045459305350615585, 'ks_p_value': 0.9597973768857192, 'ks_ci_lower': 0.003538670872882872, 'ks_ci_upper': 0.003538670872882872, 'ks_effect_size': 100725.96621786864, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 0.5728029518293755, 'ad_p_value': 1.0, 'ad_ci_lower': 14.493893604052408, 'ad_ci_upper': 14.966304545146736, 'ad_effect_size': -56.939542024477355}\n",
      "Calculating for group: ('emotion_prompt',)\n",
      "Synthetic data shape: (16689, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('emotion_prompt',): {'n_samples': 16689, 'ks_statistic': 0.0035690693683210514, 'ks_p_value': 0.9747737155162307, 'ks_ci_lower': 0.0035690693683210514, 'ks_ci_upper': 0.0035690693683210514, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 1.3210599405980614, 'ad_p_value': 1.0, 'ad_ci_lower': 17.785946840912942, 'ad_ci_upper': 17.78875908367528, 'ad_effect_size': -11124.913240126149}\n",
      "Calculating for group: ('generated_knowledge',)\n",
      "Synthetic data shape: (15914, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('generated_knowledge',): {'n_samples': 15914, 'ks_statistic': 0.003560315232330666, 'ks_p_value': 0.9749680756109433, 'ks_ci_lower': 0.0035853163542230683, 'ks_ci_upper': 0.004535358986134351, 'ks_effect_size': -1.0000000000000004, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 1.80035182552333, 'ad_p_value': 1.0, 'ad_ci_lower': 17.082005518257894, 'ad_ci_upper': 17.53071505994511, 'ad_effect_size': -65.65810027132267}\n",
      "Calculating for group: ('least_to_most',)\n",
      "Synthetic data shape: (12193, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('least_to_most',): {'n_samples': 12193, 'ks_statistic': 0.004522007072904537, 'ks_p_value': 0.9606685585625474, 'ks_ci_lower': 0.00352826621447877, 'ks_ci_upper': 0.004496526538073108, 'ks_effect_size': 0.9999999999999996, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 3.5330444999779935, 'ad_p_value': 1.0, 'ad_ci_lower': 15.043727425695208, 'ad_ci_upper': 15.27992468212493, 'ad_effect_size': -93.54335984442265}\n",
      "Calculating for group: ('recursive_criticism_and_improvement',)\n",
      "Synthetic data shape: (12317, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('recursive_criticism_and_improvement',): {'n_samples': 12317, 'ks_statistic': 0.004523658411629859, 'ks_p_value': 0.960609126258196, 'ks_ci_lower': 0.0035307225808326875, 'ks_ci_upper': 0.004498198518532496, 'ks_effect_size': 0.9999999999999996, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 5.267843588310055, 'ad_p_value': 1.0, 'ad_ci_lower': 17.43656355286234, 'ad_ci_upper': 18.036561515076013, 'ad_effect_size': -39.484410762574434}\n",
      "Calculating for group: ('rephrase_and_respond',)\n",
      "Synthetic data shape: (16973, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('rephrase_and_respond',): {'n_samples': 16973, 'ks_statistic': 0.004568201406600294, 'ks_p_value': 0.9589663239986521, 'ks_ci_lower': 0.0035720771801909766, 'ks_ci_upper': 0.0035720771801909766, 'ks_effect_size': 99612.42264093176, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 2.315273378929313, 'ad_p_value': 1.0, 'ad_ci_lower': 19.427616032413216, 'ad_ci_upper': 19.495220268037997, 'ad_effect_size': -481.8880764553989}\n",
      "Calculating for group: ('reverse_prompting',)\n",
      "Synthetic data shape: (9721, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('reverse_prompting',): {'n_samples': 9721, 'ks_statistic': 0.004480295463680356, 'ks_p_value': 0.9621363050176659, 'ks_ci_lower': 0.004480295463680356, 'ks_ci_upper': 0.004480295463680356, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 3.882805531935691, 'ad_p_value': 1.0, 'ad_ci_lower': 12.853974862690697, 'ad_ci_upper': 13.197858997805122, 'ad_effect_size': -50.516758067402236}\n",
      "Calculating for group: ('self_consistency',)\n",
      "Synthetic data shape: (14979, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Results for group ('self_consistency',): {'n_samples': 14979, 'ks_statistic': 0.004552515352170952, 'ks_p_value': 0.959553702276515, 'ks_ci_lower': 0.004552515352170952, 'ks_ci_upper': 0.004552515352170952, 'ks_effect_size': 0.0, 'kl_statistic': 1e-10, 'kl_p_value': 1.0, 'kl_ci_lower': 1e-10, 'kl_ci_upper': 1e-10, 'kl_effect_size': 0.0, 'ad_statistic': 1.1834271904052933, 'ad_p_value': 1.0, 'ad_ci_lower': 15.686566677817044, 'ad_ci_upper': 15.774985372955497, 'ad_effect_size': -312.60315189206733}\n",
      "Calculating for group: ('skeleton_of_thought',)\n",
      "Synthetic data shape: (14250, 7)\n",
      "Original data shape: (1067, 7)\n",
      "Calculating KS Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating KL Test\n",
      "Calculating Boostrap Distributions for CI and Effect Size...\n",
      "Calculating Anderson-Darling Test\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "features = ['target_thickness', 'pulse_width', 'energy', 'spot_size', 'intensity', 'power', 'cutoff_energy']\n",
    "missing_cols = set(df_synth_raw.columns) - set(df_original.columns)\n",
    "for col in missing_cols:\n",
    "    df_original[col] = np.nan\n",
    "\n",
    "df_synth_raw['source'] = 'synthetic'\n",
    "df_original['source'] = 'original'\n",
    "\n",
    "df_synth_raw_copy = df_synth_raw.copy()\n",
    "df_synth_raw_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_synth_raw_copy.dropna(subset=features, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([df_synth_raw_copy, df_original], ignore_index=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_df[features])\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features)\n",
    "scaled_df['source'] = combined_df['source'].values\n",
    "scaled_df['model'] = combined_df['model'].values\n",
    "scaled_df['prompt_method'] = combined_df['prompt_method'].values\n",
    "scaled_df['sample_size'] = combined_df['sample_size'].values\n",
    "\n",
    "# Calculate distances for different groupings\n",
    "results_model = calculate_distances(scaled_df, ['model'], features)\n",
    "results_prompt = calculate_distances(scaled_df, ['prompt_method'], features)\n",
    "results_sample = calculate_distances(scaled_df, ['sample_size'], features)\n",
    "\n",
    "results_model_prompt = calculate_distances(scaled_df, ['model', 'prompt_method'], features)\n",
    "results_model_sample_size = calculate_distances(scaled_df, ['model', 'sample_size'], features)\n",
    "\n",
    "results_model_prompt_sample = calculate_distances(scaled_df, ['model', 'prompt_method', 'sample_size'], features)\n",
    "\n",
    "# Convert results to DataFrames\n",
    "df_model = nested_dict_to_df(results_model, ['model'])\n",
    "df_prompt = nested_dict_to_df(results_prompt, ['prompt_method'])\n",
    "df_sample = nested_dict_to_df(results_sample, ['sample_size'])\n",
    "\n",
    "df_model_prompt = nested_dict_to_df(results_model_prompt, ['model', 'prompt_method'])\n",
    "df_model_sample_size = nested_dict_to_df(results_model_sample_size, ['model', 'sample_size'])\n",
    "\n",
    "df_model_prompt_sample = nested_dict_to_df(results_model_prompt_sample, ['model', 'prompt_method', 'sample_size'])\n",
    "\n",
    "# Apply significance indicators to all result DataFrames\n",
    "df_model = add_significance(df_model)\n",
    "df_prompt = add_significance(df_prompt)\n",
    "df_sample = add_significance(df_sample)\n",
    "df_model_prompt = add_significance(df_model_prompt)\n",
    "df_model_sample_size = add_significance(df_model_sample_size)\n",
    "df_model_prompt_sample = add_significance(df_model_prompt_sample)\n",
    "\n",
    "# Convert n_samples to integer\n",
    "for df in [df_model, df_model_prompt, df_model_prompt_sample]:\n",
    "    df['n_samples'] = df['n_samples'].astype(int)\n",
    "\n",
    "# Sort DataFrames by AD Statistic\n",
    "df_model_sorted = df_model.sort_values('AD Statistic')\n",
    "df_prompt_sorted = df_prompt.sort_values('AD Statistic')\n",
    "df_sample_size_sorted = df_sample.sort_values('AD Statistic')\n",
    "\n",
    "df_model_prompt_sorted = df_model_prompt.sort_values('AD Statistic')\n",
    "df_model_sample_size_sorted = df_model_sample_size.sort_values('AD Statistic')\n",
    "\n",
    "df_model_prompt_sample_sorted = df_model_prompt_sample.sort_values('AD Statistic')\n",
    "\n",
    "# Print results with significance indicators\n",
    "print(\"Results grouped by model:\")\n",
    "print(df_model_sorted)\n",
    "print(\"Results grouped by prompt:\")\n",
    "print(df_prompt_sorted)\n",
    "print(\"Results grouped by sample_size:\")\n",
    "print(df_sample_size_sorted)\n",
    "\n",
    "print(\"\\nResults grouped by model and prompt_method:\")\n",
    "print(df_model_prompt_sorted)\n",
    "print(\"\\nResults grouped by model and sample_size:\")\n",
    "print(df_model_sample_size_sorted)\n",
    "\n",
    "print(\"\\nResults grouped by model, prompt_method, and sample_size:\")\n",
    "print(df_model_prompt_sample_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
